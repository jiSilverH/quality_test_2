# ì—­ëŸ‰í‰ê°€ ì •ë¦¬

### ë¬¸ì œ9

ìì—°ì–´ì²˜ë¦¬ ëª¨ë¸ ê´€ë ¨ ì„¤ëª…ìœ¼ë¡œ ì˜³ì€ ê²ƒ ê³ ë¥´ê¸°

## 0. GPT

- Transformer decoder êµ¬ì¡°
- ê° ë¸”ë¡:
    1. Masked Multi-Head Self-Attention
    2. Feed-Forward Network (FFN)
    3. Layer Normalization & Residual Connection
- **ì‚¬ì „ í•™ìŠµ ëª©í‘œ**: ì´ì „ í† í°ë“¤ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ìŒ í† í° í™•ë¥  ë¶„í¬ë¥¼ ì˜ˆì¸¡ (ì–¸ì–´ëª¨ë¸ë§)
- ë‹¤ìŒì— ì˜¬ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì‚¬ì „ í•™ìŠµ ìˆ˜í–‰ â†’ cross-entropy loss ì‚¬ìš©

## **1. BERT (Bidirectional Encoder Representations from Transformers)**

- **êµ¬ì¡°**: Transformer **Encoder**ë§Œ ì‚¬ìš© (ì–‘ë°©í–¥ ë¬¸ë§¥ í•™ìŠµ)
- **ì‚¬ì „ í•™ìŠµ ë°©ë²•**
    1. **Masked Language Modeling (MLM)**
        - ì…ë ¥ í† í°ì˜ ì¼ë¶€ë¥¼ `[MASK]`ë¡œ ê°€ë¦¬ê³  ì›ë˜ ë‹¨ì–´ ì˜ˆì¸¡
        - ì–‘ë°©í–¥ ë¬¸ë§¥ ì •ë³´ ì‚¬ìš© ê°€ëŠ¥
    2. **Next Sentence Prediction (NSP)**
        - ë‘ ë¬¸ì¥ì´ ìˆœì„œìƒ ì´ì–´ì§€ëŠ”ì§€(True) ë˜ëŠ” ë¬´ì‘ìœ„ ì¡°í•©(False)ì¸ì§€ ë¶„ë¥˜
- **ì¥ì **: ë¬¸ì¥ ë‚´Â·ë¬¸ì¥ ê°„ ê´€ê³„ ëª¨ë‘ í•™ìŠµ ê°€ëŠ¥
- **í•œê³„**: MLM ì‹œ [MASK] í† í°ì´ ì‹¤ì œ ì‚¬ìš© í™˜ê²½ê³¼ ë¶ˆì¼ì¹˜(ë…¸ì´ì¦ˆ)

---

## **2. SpanBERT**

- **ëª©í‘œ**: **ì—°ì†ëœ êµ¬ê°„(Span)** ë‹¨ìœ„ ì˜ˆì¸¡ ì„±ëŠ¥ ê°•í™”
- **ë³€ê²½ì **
    - MLM ëŒ€ì‹  **Span Masking** ì‚¬ìš©
        
        â†’ ì„ì˜ì˜ ì—°ì†ëœ í† í° êµ¬ê°„ì„ ê°€ë ¤ì„œ êµ¬ê°„ ì „ì²´ë¥¼ ì˜ˆì¸¡
        
    - **Span Boundary Objective**: êµ¬ê°„ ì–‘ ëì˜ representationì„ í†µí•´ ë‚´ë¶€ í† í° ë³µì›
- **ì¥ì **: ê°œì²´ëª… ì¸ì‹, ì§ˆì˜ì‘ë‹µ(QA)ì²˜ëŸ¼ **ì—°ì† êµ¬ê°„ ì˜ˆì¸¡** íƒœìŠ¤í¬ì—ì„œ BERTë³´ë‹¤ ìš°ìˆ˜

---

## **3. RoBERTa (Robustly Optimized BERT)**

- **ê°œì„  í¬ì¸íŠ¸**
    1. NSP ì œê±° â†’ ì˜¤íˆë ¤ ì„±ëŠ¥ í–¥ìƒ
    2. í•™ìŠµ ë°ì´í„°ì™€ ë°°ì¹˜ í¬ê¸°, í•™ìŠµ ì‹œê°„ **ëŒ€í­ ì¦ê°€**
    3. Dynamic Masking â†’ ë§¤ epochë§ˆë‹¤ ë‹¤ë¥¸ ë§ˆìŠ¤í‚¹ íŒ¨í„´ ì ìš© = ë°ì´í„°ë¥¼ ë¡œë”©í•  ë•Œë§ˆë‹¤ ë§ˆìŠ¤í‚¹ ìœ„ì¹˜ë¥¼ ë°”ê¾¸ëŠ” ê²ƒì„ ì˜ë¯¸
        1. bertëŠ” static maskingì„
    4. ë” ê¸´ ì‹œí€€ìŠ¤ ê¸¸ì´ë¡œ í•™ìŠµ
- **íš¨ê³¼**: ë™ì¼ êµ¬ì¡°ë¡œ BERTë³´ë‹¤ ì„±ëŠ¥ í–¥ìƒ

---

## **4. ELECTRA**

- **ì•„ì´ë””ì–´**: "í† í° ì˜ˆì¸¡" ëŒ€ì‹  "ì§„ì§œ vs ê°€ì§œ" íŒë³„
- **Generatorâ€“Discriminator êµ¬ì¡°**
    1. **Generator**: MLM ë°©ì‹ìœ¼ë¡œ ê°€ì§œ í† í° ìƒì„±
    2. **Discriminator**: ê° **í† í°**ì´ ì›ë˜ ì§„ì§œì˜€ëŠ”ì§€(Real) ì•„ë‹ˆë©´ Generatorê°€ ë§Œë“  ê°€ì§œì¸ì§€(Fake) íŒë³„
- **ì¥ì **
    - ëª¨ë“  í† í°ì—ì„œ í•™ìŠµ ì‹ í˜¸ ì œê³µ (MLMë³´ë‹¤ íš¨ìœ¨ì )
    - ë” ì ì€ ê³„ì‚°ëŸ‰ìœ¼ë¡œë„ BERT ìˆ˜ì¤€Â·ì´ìƒì˜ ì„±ëŠ¥

## **5. BART (Bidirectional and Auto-Regressive Transformers)**

### 1. êµ¬ì¡° ê°œìš”

- **Transformer ê¸°ë°˜ Encoderâ€“Decoder êµ¬ì¡°**
    - **Encoder**: BERTì²˜ëŸ¼ **ì–‘ë°©í–¥** ë¬¸ë§¥ ì´í•´
    - **Decoder**: GPTì²˜ëŸ¼ **ì™¼ìª½ì—ì„œ ì˜¤ë¥¸ìª½**(Auto-regressive) ìƒì„±
- ì…ë ¥ì„ **ë‹¤ì–‘í•œ ë…¸ì´ì¦ˆë¡œ ë³€í˜•**í•œ í›„, ì›ë˜ ë¬¸ì¥ì„ ë³µì›í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ í•™ìŠµ

---

### 2. ì‚¬ì „ í•™ìŠµ ë°©ì‹ (Denoising Autoencoder)

BARTëŠ” ì›ë¬¸ â†’ **ë…¸ì´ì¦ˆ ì¶”ê°€** â†’ ë³µì›í•˜ëŠ” ê³¼ì •ì„ í•™ìŠµ

- **ë…¸ì´ì¦ˆ ìœ í˜•**
    1. **Token Deletion**: í† í° ì œê±°; ë¬´ì‘ìœ„ë¡œ í† í°ì„ ì‚­ì œí•˜ëŠ” ë°©ë²•, ëª¨ë¸ì€ ì–´ë””ì—ì„œ í† í°ì´ ëˆ„ë½ë˜ì—ˆëŠ”ì§€ë¥¼ ê²°ì •í•´ì•¼í•¨
    2. **Text Infilling**: ì—°ì†ëœ êµ¬ê°„ì„ ë§ˆìŠ¤í‚¹, ê° spanì—ì„œ ì–¼ë§ˆë§Œí¼ì˜ í† í°ë“¤ì´ ì—†ì–´ì¡ŒëŠ”ì§€ë¥¼ í•™ìŠµ
    3. **Sentence Permutation**: ë¬¸ì¥ ìˆœì„œ ì„ê¸°
    4. **Token Masking**: ì¼ë¶€ í† í°ì„ `[MASK]`ë¡œ ë°”ê¿ˆ
    5. **Document Rotation**: ë¬¸ì„œ ì‹œì‘ ìœ„ì¹˜ë¥¼ ë°”ê¿ˆ
- **ëª©í‘œ**: Decoderê°€ ì›ë˜ ë¬¸ì¥ì„ ì¬êµ¬ì„±

---

### 3. íŠ¹ì§•

- **ë‹¤ëª©ì ì„±**: ìš”ì•½, ë²ˆì—­, ì§ˆì˜ì‘ë‹µ, ë¬¸ì¥ ìƒì„± ëª¨ë‘ ê°€ëŠ¥
- **ì¥ì **
    - Encoder ë•ë¶„ì— ì…ë ¥ ì „ì²´ì˜ ì–‘ë°©í–¥ ì´í•´
    - Decoder ë•ë¶„ì— ë¬¸ì¥ ìƒì„± ëŠ¥ë ¥ ìš°ìˆ˜
    - ë‹¤ì–‘í•œ ë…¸ì´ì¦ˆ ë•ë¶„ì— ë³µì›Â·ìƒì„± ëŠ¥ë ¥ ê°•í™”
- **í™œìš© ì˜ˆì‹œ**:
    - ìš”ì•½(Summarization)
    - ë°ì´í„° ë³µì›(ì—ëŸ¬ ìˆ˜ì •)
    - ìì—°ìŠ¤ëŸ¬ìš´ í…ìŠ¤íŠ¸ ìƒì„±

---

### 4. ë‹¤ë¥¸ ëª¨ë¸ê³¼ ë¹„êµ

| ëª¨ë¸ | êµ¬ì¡° | í•™ìŠµ ë°©ì‹ | ì£¼ìš” íŠ¹ì§• |
| --- | --- | --- | --- |
| **BERT** | Encoderë§Œ | MLM + NSP | ì´í•´ ì¤‘ì‹¬ |
| **GPT** | Decoderë§Œ | Auto-regressive LM | ìƒì„± ì¤‘ì‹¬ |
| **BART** | Encoder-Decoder | Denoising Autoencoder | ì´í•´ + ìƒì„± ë‘˜ ë‹¤ ê°€ëŠ¥ |

### ë¬¸í•­10

LoRA (Row-rank Adaptation)

- LLMì˜ íŒŒë¼ë¯¸í„° íš¨ìœ¨ì„±ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´, ì „ì²´ ê°€ì¤‘ì¹˜ í–‰ë ¬ì„ ì§ì ‘ ìˆ˜ì •í•˜ëŠ” ëŒ€ì‹ , ë‘ ê°œì˜ ì €ì°¨ì› í–‰ë ¬ Aì™€ í–‰ë ¬ Bë¥¼ ì‚¬ìš©
- ì´ í–‰ë ¬ë“¤ì€ ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ì˜ ê° transformer layerì— ì‚½ì…ë˜ì–´, Aì™€ Bì˜ ê³± BAê°€ ì›ë˜ì˜ ê°€ì¤‘ì¹˜ W_0ì— ë”í•´ì ¸ì„œ, ê°€ì¤‘ì¹˜ì˜ ë³€í™” delta_Wë¥¼ ëª¨ë¸ë§í•œë‹¤.
- ì´ ì €ì°¨ì› ê³±ì…ˆì€ ë³¸ì§ˆì ìœ¼ë¡œ ëª¨ë¸ì˜ í‘œí˜„ë ¥ì„ ì¦ê°€ì‹œí‚¤ë©´ì„œë„ íŒŒë¼ë¯¸í„° ìˆ˜ëŠ” ìƒëŒ€ì ìœ¼ë¡œ ì ê²Œ ì¦ê°€ì‹œì¼œ, ì „ì²´ ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„°ë¥¼ í¬ê²Œ ëŠ˜ë¦¬ì§€ ì•Šê³ ë„ íŠ¹ì • ì‘ì—…ì— ë” íš¨ê³¼ì ìœ¼ë¡œ ì ì‘í•  ìˆ˜ ìˆë„ë¡ ë•ëŠ”ë‹¤

## **LoRA (Low-Rank Adaptation)**

### 1. ê°œë…

- **ëŒ€ê·œëª¨ ëª¨ë¸ì˜ ëª¨ë“  íŒŒë¼ë¯¸í„°ë¥¼ ì—…ë°ì´íŠ¸í•˜ì§€ ì•Šê³ **, ì¼ë¶€ ê°€ì¤‘ì¹˜ í–‰ë ¬ì„ **ì €ì°¨ì›(ë‚®ì€ rank) í–‰ë ¬ ë¶„í•´** í˜•íƒœë¡œ í•™ìŠµí•˜ëŠ” ê¸°ë²•
- ì›ë˜ ëª¨ë¸ íŒŒë¼ë¯¸í„°ëŠ” **ë™ê²°(freeze)**, ìƒˆë¡œ ì¶”ê°€í•œ LoRA ëª¨ë“ˆë§Œ í•™ìŠµ

---

### 2. ë™ì‘ ì›ë¦¬

- Transformerì˜ **Attention ê°€ì¤‘ì¹˜ í–‰ë ¬ W**ë¥¼ ì§ì ‘ ì—…ë°ì´íŠ¸í•˜ì§€ ì•ŠìŒ
- ëŒ€ì‹  **W + Î”W** í˜•íƒœë¡œ ì“°ë˜,
    - Î”W = **A Ã— B**
    - A, BëŠ” rankê°€ ë‚®ì€(ì‘ì€ ì°¨ì›ì˜) í•™ìŠµ ê°€ëŠ¥ í–‰ë ¬
- ìˆ˜ì‹:
    
    ```
    y = Wx
    y' = (W + BA)x
    
    ```
    
    - W: ì›ë˜ ëª¨ë¸ì˜ ê³ ì •ëœ ê°€ì¤‘ì¹˜
    - B, A: í•™ìŠµë˜ëŠ” ì‘ì€ í–‰ë ¬ (rank r, r << dimension)

---

### 3. ì¥ì 

1. **íŒŒë¼ë¯¸í„° íš¨ìœ¨**: í•™ìŠµí•´ì•¼ í•˜ëŠ” íŒŒë¼ë¯¸í„° ìˆ˜ê°€ ë§¤ìš° ì ìŒ â†’ ë©”ëª¨ë¦¬ ì ˆì•½
2. **ì €ì¥Â·ë°°í¬ ìš©ì´**: LoRA ëª¨ë“ˆë§Œ ì €ì¥í•˜ë©´ ë¨ (ì›ë³¸ ëª¨ë¸ ê³µìœ  ê°€ëŠ¥)
3. **ì„±ëŠ¥ ìœ ì§€**: í’€ íŒŒì¸íŠœë‹ì— ê°€ê¹Œìš´ ì„±ëŠ¥
4. **ë‹¤ì¤‘ íƒœìŠ¤í¬ ì „í™˜**: í•˜ë‚˜ì˜ ëª¨ë¸ì— ì—¬ëŸ¬ LoRA ëª¨ë“ˆì„ ë¶™ì—¬ ì‰½ê²Œ íƒœìŠ¤í¬ ë³€ê²½ ê°€ëŠ¥

---

### 4. ì‚¬ìš© ì˜ˆì‹œ

- **ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)** â†’ GPT, LLaMA, BLOOM, Falcon ë“± íŒŒì¸íŠœë‹
- **ë©€í‹°íƒœìŠ¤í¬ í•™ìŠµ** â†’ ê° íƒœìŠ¤í¬ë§ˆë‹¤ LoRA ëª¨ë“ˆ ë”°ë¡œ ì €ì¥
- **ê²½ëŸ‰ ë””ë°”ì´ìŠ¤**ì—ì„œì˜ íŒŒì¸íŠœë‹ â†’ VRAM ì ì€ GPUì—ì„œë„ ê°€ëŠ¥

### ë¬¸í•­11

## 0. CAM (class activation mapping)

## **1. ê°œë…**

- CNN ê¸°ë°˜ ë¶„ë¥˜ ëª¨ë¸ì´ íŠ¹ì • í´ë˜ìŠ¤ë¥¼ ì˜ˆì¸¡í•  ë•Œ, **ì´ë¯¸ì§€ì˜ ì–´ëŠ ë¶€ë¶„ì´ ì¤‘ìš”í•œì§€**ë¥¼ ë³´ì—¬ì£¼ëŠ” ê¸°ìˆ 
- **í´ë˜ìŠ¤ë³„ í™œì„±í™” ì§€ë„(Class Activation Map)** ìƒì„±
- ì£¼ë¡œ **ëª¨ë¸ í•´ì„(Explainability)**, **ëª¨ë¸ ì‹ ë¢°ì„± í‰ê°€** ë“±ì— ì‚¬ìš©

---

## **2. ì›ë¦¬ (CAM, ì›ë˜ ë°©ì‹)**

![image.png](%EC%97%AD%EB%9F%89%ED%8F%89%EA%B0%80%20%EC%A0%95%EB%A6%AC%2024f4d6b253d380f4a698df83390921af/image.png)

- êµ¬ì¡° ì „ì œ: **Global Average Pooling (GAP)** + **FC Layer(ë¶„ë¥˜ê¸°)**
- ê³¼ì •:
    1. **ë§ˆì§€ë§‰ í•©ì„±ê³±(Conv) ë ˆì´ì–´**ì—ì„œ feature map f_k(x, y) ì¶”ì¶œ
        - k: ì±„ë„ ì¸ë±ìŠ¤
    2. GAPì„ ì ìš©í•˜ì—¬ ì±„ë„ë³„ í‰ê· ê°’ F_k ê³„ì‚°
    3. Fully Connected Layerì˜ í´ë˜ìŠ¤ë³„ ê°€ì¤‘ì¹˜ w_{c,k} ì‚¬ìš©
    4. íŠ¹ì • í´ë˜ìŠ¤ cì— ëŒ€í•œ CAM ê³„ì‚°:Mc(x,y)=kâˆ‘wc,kâ‹…fk(x,y)
        
        Mc(x,y)=âˆ‘kwc,kâ‹…fk(x,y)M_c(x, y) = \sum_k w_{c,k} \cdot f_k(x, y)
        
    5. McM_cMcë¥¼ ì›ë³¸ ì´ë¯¸ì§€ í¬ê¸°ë¡œ ë³´ê°„ â†’ ì‹œê°í™”
- **GAP will calculate a single value for an entire feature map**

---

## **3. í•œê³„**

- **GAP + FC êµ¬ì¡°**ì—¬ì•¼ë§Œ ì ìš© ê°€ëŠ¥
- êµ¬ì¡° ì œì•½ ë•Œë¬¸ì— ResNet, EfficientNet ë“± ì¼ë°˜ êµ¬ì¡°ì— ë°”ë¡œ ì ìš© ì–´ë ¤ì›€ â†’ ì´í›„ **Grad-CAM** ê°™ì€ ë³€í˜• ê¸°ë²• ë“±ì¥

---

## **4. ë³€í˜• ê¸°ë²• ì˜ˆì‹œ**

- **Grad-CAM**: êµ¬ì¡° ì œì•½ ì—†ì´ ì‚¬ìš© ê°€ëŠ¥, gradientë¥¼ í™œìš©í•´ ì¤‘ìš”ë„ ê³„ì‚°
- **Score-CAM**: gradient ì—†ì´ score ë³€í™”ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì¤‘ìš”ë„ ê³„ì‚°

---

## **5. í™œìš© ë¶„ì•¼**

- ëª¨ë¸ í•´ì„ ë° ë””ë²„ê¹…
- ì˜ë£Œ ì˜ìƒ ë¶„ì„(ë³‘ë³€ ìœ„ì¹˜ í™•ì¸)
- ê°ì²´ ì¸ì‹ ëª¨ë¸ ì‹œê°í™”
- ë°ì´í„°ì…‹ ì˜¤ë¥˜ íƒì§€

---

ğŸ“Œ **ì•”ê¸° íŒ**

- CAM = **(FC Layer weight) Ã— (Feature map)**
- GAP í•„ìˆ˜ â†’ êµ¬ì¡° ì œí•œ
- ë³€í˜•: Grad-CAM(gradient), Score-CAM(score ë³€í™”)

## **1. Grad-CAM (Gradient-weighted Class Activation Mapping)**

### ê°œë…

- **CAMì˜ êµ¬ì¡°ì  ì œì•½**(GAP+FC í•„ìš”)ì„ ì—†ì•¤ ë²„ì „
- **Gradient**ë¥¼ ì´ìš©í•´ ë§ˆì§€ë§‰ Convolution Layerì˜ feature map ì±„ë„ë³„ ì¤‘ìš”ë„ë¥¼ ê³„ì‚°
    - ê° feature mapì˜ ëª¨ë“  ì›ì†Œë¥¼ ë¯¸ë¶„í•œ ê²ƒì„ ë”í•œ ë‹¤ìŒ ì „ì²´ í¬ê¸°ë¡œ ë‚˜ëˆ  ê° feature mapì— ëŒ€í•œ gradientë¥¼ êµ¬í•¨
    - ì´ gradientë¥¼ ê°€ì¤‘ í•©í•˜ì—¬
- CNN, ResNet, VGG, Inception ë“± ëŒ€ë¶€ë¶„ì˜ êµ¬ì¡°ì— ì ìš© ê°€ëŠ¥
- **feature map(íŠ¹ì§•ë§µ)ì— ëŒ€í•œ ì†ì‹¤í•¨ìˆ˜(loss function)ì— ëŒ€í•œ ê¸°ìš¸ê¸°ë¥¼ ê¸°ë°˜**ìœ¼ë¡œ íŠ¹ì§•ë§µì˜ ì¤‘ìš”ë„ë¥¼ êµ¬í•˜ëŠ” ë°©ë²•

### ì›ë¦¬

![image.png](%EC%97%AD%EB%9F%89%ED%8F%89%EA%B0%80%20%EC%A0%95%EB%A6%AC%2024f4d6b253d380f4a698df83390921af/image%201.png)

### íŠ¹ì§•

- êµ¬ì¡° ì œì•½ ì—†ìŒ
- Gradient ê¸°ë°˜ì´ë¯€ë¡œ **ì—­ì „íŒŒ í•„ìš”**
- ReLU ì‚¬ìš© â†’ ì–‘ì˜ ê¸°ì—¬ë§Œ ê°•ì¡°

---

## **2. Score-CAM**

### ê°œë…

- Gradient ì‚¬ìš© ì—†ì´ **ì¶œë ¥ score ë³€í™”**ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì¤‘ìš”ë„ ê³„ì‚°
- CAMì˜ noise ë¬¸ì œì™€ Grad-CAMì˜ gradient ì˜ì¡´ì„± ë¬¸ì œ ì™„í™”

### ì›ë¦¬

![image.png](%EC%97%AD%EB%9F%89%ED%8F%89%EA%B0%80%20%EC%A0%95%EB%A6%AC%2024f4d6b253d380f4a698df83390921af/image%202.png)

### íŠ¹ì§•

- Gradient ë¶ˆí•„ìš” â†’ ë°±í”„ë¡œí¼ê²Œì´ì…˜ ì—†ì´ ê°€ëŠ¥
- ê³„ì‚°ëŸ‰ ë§ìŒ (ì±„ë„ ìˆ˜ë§Œí¼ forward í•„ìš”)
- ë…¸ì´ì¦ˆ ì ê³  ì•ˆì •ì 

---

## **3. ë¹„êµ ìš”ì•½ í‘œ**

| í•­ëª© | Grad-CAM | Score-CAM |
| --- | --- | --- |
| **Gradient í•„ìš”** | âœ” í•„ìš” | âœ˜ ë¶ˆí•„ìš” |
| **ê³„ì‚°ëŸ‰** | ì ìŒ | ë§ìŒ (ì±„ë„ ìˆ˜ Ã— forward) |
| **ë…¸ì´ì¦ˆ** | ë‹¤ì†Œ ìˆìŒ | ì ìŒ |
| **ì ìš© êµ¬ì¡°** | ì œì•½ ì—†ìŒ | ì œì•½ ì—†ìŒ |
| **ì›ë¦¬** | gradientë¡œ ì±„ë„ ê°€ì¤‘ì¹˜ | score ë³€í™”ë¡œ ì±„ë„ ê°€ì¤‘ì¹˜ |

---

ğŸ“Œ **ì•”ê¸° íŒ**

- **Grad-CAM** = gradientë¡œ ê°€ì¤‘ì¹˜ â†’ ë¹ ë¦„, noise ê°€ëŠ¥
- **Score-CAM** = score ë³€í™”ë¡œ ê°€ì¤‘ì¹˜ â†’ ëŠë¦¼, noise ì ìŒ

### ë¬¸í•­12

XAI

**Logistic regression**

![image.png](%EC%97%AD%EB%9F%89%ED%8F%89%EA%B0%80%20%EC%A0%95%EB%A6%AC%2024f4d6b253d380f4a698df83390921af/image%203.png)

| í•­ëª© | Linear Regression | Logistic Regression |
| --- | --- | --- |
| ëª©ì  | ì—°ì†í˜• ê°’ ì˜ˆì¸¡ | ì´ì§„ ë¶„ë¥˜ í™•ë¥  ì˜ˆì¸¡ |
| ì¶œë ¥ ë²”ìœ„ | âˆ’âˆ-\infty ~ +âˆ+\infty | 0 ~ 1 |
| í™œì„± í•¨ìˆ˜ | ì—†ìŒ | Sigmoid |
| ì†ì‹¤ í•¨ìˆ˜ | MSE | Cross-Entropy |
| ì¶œë ¥ í•´ì„ | ì˜ˆì¸¡ê°’ ê·¸ëŒ€ë¡œ ì‚¬ìš© | í™•ë¥ ë¡œ í•´ì„, ì„ê³„ê°’ ì ìš© |

ğŸ“Œ **í•œ ì¤„ ìš”ì•½**

- Linear Regression: "ê·¸ëƒ¥ ì„ í˜• ë°©ì •ì‹" â†’ ê°’ ê·¸ëŒ€ë¡œ ì˜ˆì¸¡
- Logistic Regression: "ì„ í˜• ë°©ì •ì‹ + Sigmoid" â†’ í™•ë¥ ë¡œ ë³€í™˜í•´ ë¶„ë¥˜

- Surrogate ëª¨ë¸
    - ë³µì¡í•œ ì‹œìŠ¤í…œì´ë‚˜ ëª¨ë¸ì„ ëŒ€ì‹ í•˜ì—¬ ê·¼ì‚¬ì¹˜ë¥¼ ì œê³µí•˜ëŠ” ë³´ì¡° ëª¨ë¸
    - ë¸”ë™ë°•ìŠ¤ ëª¨ë¸ì˜ ì˜ˆì¸¡ì„ í•´ì„í•˜ê³  ì´í•´í•˜ëŠ” ë°ì— ë„ì›€ì„ ì¤Œ â†’ ë¸”ë™ë°•ìŠ¤ ëª¨ë¸ì˜ ë™ì‘ì„ ë³´ë‹¤ ì‰½ê²Œ ì´í•´ ê°€ëŠ¥!
    

### ë¬¸í•­17

### 1. **Model-in-Service**

- **ì •ì˜**: ëª¨ë¸ì´ íŠ¹ì • ì„œë¹„ìŠ¤(í˜¹ì€ ì• í”Œë¦¬ì¼€ì´ì…˜)ì˜ ì¼ë¶€ë¡œ ë‚´ì¥ë˜ì–´ ìš´ì˜ë˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤.
- **íŠ¹ì§•**:
    - ì˜ˆ: ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ ë‚´ íŠ¹ì • íŠ¸ë˜í”½ ì˜ˆì¸¡, ì¶”ì²œ ì‹œìŠ¤í…œ ë“±ì˜ ê¸°ëŠ¥ì´ ì„œë¹„ìŠ¤ ì½”ë“œ ë‚´ë¶€ì— ì§ì ‘ í¬í•¨ëœ í˜•íƒœ.
    - ëª¨ë¸ ë°°í¬ì™€ ì„œë¹„ìŠ¤ ìš´ì˜ì´ ê²°í•©ë˜ì–´ ìˆìŒ. ìœ ì§€, ë²„ì „ ê´€ë¦¬, í™•ì¥ ì±…ì„ì´ ì„œë¹„ìŠ¤ ê°œë°œìì—ê²Œ ìˆìŒ.
    - ì „í˜•ì ìœ¼ë¡œ â€œì„œë¹„ìŠ¤ ë ˆì´ì–´â€ í˜¹ì€ API ì„œë²„ ë‚´ë¶€ì— í¬í•¨ë˜ì–´ ë™ì‘í•©ë‹ˆë‹¤.
    
    *ë¹„ìŠ·í•œ ë§¥ë½ì—ì„œ â€œModelâ€ê³¼ â€œServiceâ€ ì°¨ì´ë¥¼ MVC ê¸°ì¤€ìœ¼ë¡œ ë³´ë©´:*
    
    ëª¨ë¸(Model)ì€ ë°ì´í„° êµ¬ì¡°ì™€ ì—°ì‚°ì„ ë‹´ë‹¹í•˜ê³ , ì„œë¹„ìŠ¤(Service)ëŠ” ëª¨ë¸ê³¼ ì—°ê´€ëœ ë‹¤ì–‘í•œ ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, `EmployeeService::hireEmployee`ëŠ” ì±„ìš© ì ˆì°¨ ì „ë°˜(ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±, ì´ë©”ì¼ ë°œì†¡ ë“±)ì„ ì²˜ë¦¬í•˜ì£ .
    

---

### 2. **Model-as-a-Service (MaaS)**

- **ì •ì˜**: ê¸°ê³„ í•™ìŠµ(Machine Learning) ë˜ëŠ” AI ëª¨ë¸ì„ í´ë¼ìš°ë“œ ê¸°ë°˜ìœ¼ë¡œ í˜¸ìŠ¤íŒ…í•˜ê³ , APIë¥¼ í†µí•´ ì™¸ë¶€ì—ì„œ **ì„œë¹„ìŠ¤ í˜•íƒœë¡œ ì œê³µ**í•˜ëŠ” ëª¨ë¸ ì œê³µ ë°©ì‹ì…ë‹ˆë‹¤.
- **íŠ¹ì§•**:
    - ì‚¬ì „ í•™ìŠµëœ ML ëª¨ë¸ì„ API í˜•íƒœë¡œ ì œê³µ â†’ í˜¸ì¶œë§Œ í•˜ë©´ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ë°›ì„ ìˆ˜ ìˆìŒ.
    - ì¸í”„ë¼, ë°°í¬, í™•ì¥, ìš´ì˜ì€ ëª¨ë¸ ì œê³µì(ì„œë¹„ìŠ¤) ì¸¡ì—ì„œ ê´€ë¦¬.
    - AI/ML ê¸°ëŠ¥ì„ ë¹ ë¥´ê²Œ ë„ì…í•˜ê³  ì‹¶ì€ ì¡°ì§ì— ì í•©í•˜ë©°, ë¹„ìš© ì ˆê°, í™•ì¥ì„±, ì ‘ê·¼ì„±ì„ ë™ì‹œì— í™•ë³´í•  ìˆ˜ ìˆìŒ.
    - â€œX-as-a-Serviceâ€ì˜ ì¼ì¢…ìœ¼ë¡œ, SaaSë‚˜ PaaSì²˜ëŸ¼ ì‚¬ìš©ëŸ‰ ê¸°ë°˜ êµ¬ë… ëª¨ë¸ë¡œ ì œê³µë˜ëŠ” í´ë¼ìš°ë“œí˜• AI ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤.
- **ì¶œì²˜ë³„ ì„¤ëª…**:
    - Azure ì„¤ëª…ì—ì„œëŠ”, MaaSëŠ” ì‚¬ì „ í•™ìŠµëœ ML ëª¨ë¸ì— ëŒ€í•œ "í´ë¼ìš°ë“œ ê¸°ë°˜ API ì•¡ì„¸ìŠ¤" í˜•íƒœì´ë©°, AI ê¸°ëŠ¥ì„ ë¹ ë¥´ê²Œ Appì— í†µí•©í•  ìˆ˜ ìˆê²Œ í•´ì¤€ë‹¤ê³  ê°•ì¡°í•©ë‹ˆë‹¤.[Microsoft Azure](https://azure.microsoft.com/en-us/resources/cloud-computing-dictionary/what-is-models-as-a-service-maas?utm_source=chatgpt.com)
    - Red Hatì€ MaaSë¥¼ â€œì¡°ì§ ë‚´ë¶€ì—ì„œ ëª¨ë¸ì„ íŒ€, ì• í”Œë¦¬ì¼€ì´ì…˜ ë‹¨ìœ„ë¡œ ì˜¨ë””ë§¨ë“œë¡œ ì œê³µí•  ìˆ˜ ìˆê²Œ í•˜ëŠ” ê³µìœ  ë¦¬ì†ŒìŠ¤â€ë¡œ ì •ì˜í•©ë‹ˆë‹¤. ë˜í•œ í•˜ì´ë¸Œë¦¬ë“œ í´ë¼ìš°ë“œ í™˜ê²½, API ê²Œì´íŠ¸ì›¨ì´, ì¶”ì Â·ê´€ë¦¬ë¥¼ ìœ„í•œ ì‹œìŠ¤í…œ êµ¬ì„±ë„ í¬í•¨ëœë‹¤ê³  í•©ë‹ˆë‹¤.[Red Hat](https://www.redhat.com/en/topics/ai/what-is-models-as-a-service?utm_source=chatgpt.com)
    - GenAI ì‹œëŒ€ì— MaaSëŠ” AI ëª¨ë¸ ì‚¬ìš©ì„ ë¯¼ì£¼í™”í•˜ê³  ì¸í”„ë¼ ë¶€ë‹´ ì—†ì´ AIë¥¼ í™œìš©í•˜ê²Œ ë§Œë“œëŠ” ì¤‘ìš”í•œ íŒ¨ëŸ¬ë‹¤ì„ì´ë¼ëŠ” ì ë„ ì–¸ê¸‰ë©ë‹ˆë‹¤.[ResearchGate+4arXiv+4Microsoft Azure+4](https://arxiv.org/abs/2311.05804?utm_source=chatgpt.com)

---

## ìš”ì•½ ë¹„êµí‘œ

| êµ¬ë¶„ | Model-in-Service | Model-as-a-Service (MaaS) |
| --- | --- | --- |
| **ìœ„ì¹˜ / í†µí•© ë°©ì‹** | ì„œë¹„ìŠ¤ ì½”ë“œ ë‚´ë¶€ì— í†µí•© | ë…ë¦½ì  API í˜•íƒœë¡œ ì™¸ë¶€ ì œê³µ |
| **ê´€ë¦¬ ì±…ì„** | ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œìê°€ ë‹´ë‹¹ | ëª¨ë¸ ì œê³µì/í”Œë«í¼ì—ì„œ ë‹´ë‹¹ |
| **í™•ì¥ì„± ë° ì¸í”„ë¼** | ê°œë°œìê°€ ì§ì ‘ í™•ì¥/ìš´ì˜ | í´ë¼ìš°ë“œ ê¸°ë°˜ìœ¼ë¡œ ìë™ í™•ì¥ |
| **ì‚¬ìš©ì ì ‘ê·¼ ë°©ì‹** | ë‚´ë¶€ ì‚¬ìš© ëª©ì  ìœ„ì£¼ | ë‚´ë¶€ì™¸ë¶€ ëˆ„êµ¬ë‚˜ API í˜¸ì¶œ ê°€ëŠ¥ |
| **ì í•© ëŒ€ìƒ** | íŠ¹ì • ê¸°ëŠ¥ì— í†µí•©ëœ ëª¨ë¸ | AI ê¸°ëŠ¥ì„ ë¹ ë¥´ê²Œ ë„ì…í•˜ê³  ì‹¶ì€ ì¡°ì§ |

### ë¬¸í•­ 19

## **1. Zero-Shot Learning (ZSL)**

- **ì •ì˜**: í•™ìŠµì—ì„œ **ë³¸ ì  ì—†ëŠ” í´ë˜ìŠ¤**ì— ëŒ€í•´ ì˜ˆì¸¡
- **ë°©ë²•**: í´ë˜ìŠ¤ì˜ **ì„¤ëª…(í…ìŠ¤íŠ¸)**, **ì†ì„±(attribute)**, ë˜ëŠ” **ì˜ë¯¸ë¡ ì  ì„ë² ë”©**ì„ ì‚¬ìš©
- **ì˜ˆì‹œ**: â€œì–¼ë£©ë§â€ ì´ë¯¸ì§€ë¥¼ ë³¸ ì  ì—†ì§€ë§Œ, â€œì¤„ë¬´ëŠ¬ê°€ ìˆëŠ” ë§â€ì´ë¼ëŠ” ì†ì„± ì„¤ëª…ì„ ì´ìš©í•´ ë¶„ë¥˜
- **íŠ¹ì§•**:
    - ë¼ë²¨ ë°ì´í„° ì—†ì´ ìƒˆë¡œìš´ í´ë˜ìŠ¤ë¥¼ ë‹¤ë£° ìˆ˜ ìˆìŒ
    - í´ë˜ìŠ¤ ì„¤ëª… í’ˆì§ˆì— ë”°ë¼ ì„±ëŠ¥ ì¢Œìš°ë¨

---

## **2. One-Shot Learning**

- **ì •ì˜**: ìƒˆë¡œìš´ í´ë˜ìŠ¤ë‹¹ **1ê°œì˜ ìƒ˜í”Œ**ë§Œ ë³´ê³  í•™ìŠµ/ì¶”ë¡ 
- **ë°©ë²•**: ì£¼ë¡œ **metric learning** ë˜ëŠ” **Siamese network**ë¥¼ ì‚¬ìš©í•´ ì´ë¯¸ì§€ ê°„ ìœ ì‚¬ë„ ê³„ì‚°
- **ì˜ˆì‹œ**: ì‹ ë¶„ì¦ ì¸ì‹ ì‹œìŠ¤í…œì—ì„œ, íŠ¹ì • ì‚¬ëŒì˜ ì‚¬ì§„ 1ì¥ë§Œìœ¼ë¡œ ê·¸ ì‚¬ëŒì„ ì¸ì‹
- **íŠ¹ì§•**:
    - ë°ì´í„°ê°€ ë§¤ìš° ì œí•œëœ í™˜ê²½ì—ì„œ ìœ ìš©
    - ì¼ë°˜í™” ëŠ¥ë ¥ì´ ì¤‘ìš”

---

## **3. Few-Shot Learning (FSL)**

- **ì •ì˜**: ìƒˆë¡œìš´ í´ë˜ìŠ¤ë‹¹ **ì†ŒëŸ‰(ìˆ˜~ìˆ˜ì‹­ ê°œ)ì˜ ìƒ˜í”Œ**ë¡œ í•™ìŠµ
- **ë°©ë²•**: Meta-Learning, Prompt-based Learning ë“± ì‚¬ìš©
- **ì˜ˆì‹œ**:
    - 5ê°œì˜ ê³ ì–‘ì´ í’ˆì¢… ì‚¬ì§„ì„ ë³´ê³  í’ˆì¢… ë¶„ë¥˜ í•™ìŠµ
- **íŠ¹ì§•**:
    - ZSLë³´ë‹¤ ë°ì´í„°ê°€ ì¡°ê¸ˆ ë” ìˆìŒ
    - ëŒ€ê·œëª¨ ì‚¬ì „í•™ìŠµ ëª¨ë¸ì„ í™œìš©í•˜ë©´ íš¨ê³¼ â†‘

---

## **4. Generalized Zero-Shot Learning (GZSL)**

- **ì •ì˜**: í…ŒìŠ¤íŠ¸ ì‹œ **ë³¸ ì  ìˆëŠ” í´ë˜ìŠ¤(Seen)** + **ë³¸ ì  ì—†ëŠ” í´ë˜ìŠ¤(Unseen)**ë¥¼ **ëª¨ë‘** êµ¬ë¶„í•´ì•¼ í•˜ëŠ” ì„¤ì •
    
    â†’ zero-shot learningë³´ë‹¤ ë” ì‹¤ì œì ì¸ ì‹œë‚˜ë¦¬ì˜¤ì— ê°€ê¹Œì›€, ëª¨ë¸ì˜ ì¼ë°˜í™” ëŠ¥ë ¥ì„ í¬ê²Œ í–¥ìƒ ê°€ëŠ¥
    
- **ì°¨ì´ì **:
    - ZSLì€ â€œUnseen í´ë˜ìŠ¤ë§Œâ€ ë¶„ë¥˜
    - GZSLì€ Seen/Unseen ëª¨ë‘ ì˜ˆì¸¡í•´ì•¼ í•˜ë¯€ë¡œ, Seen í´ë˜ìŠ¤ ì ë¦¼(bias) ë¬¸ì œ ë°œìƒ
- **ì˜ˆì‹œ**:
    - í•™ìŠµ: ê³ ì–‘ì´, ê°œ (Seen)
    - í…ŒìŠ¤íŠ¸: ê³ ì–‘ì´, ê°œ, í˜¸ë‘ì´, ì–¼ë£©ë§ (Seen+Unseen)

---

## **5. ë¹„êµ í‘œ**

| êµ¬ë¶„ | í•™ìŠµ ì‹œ ìƒˆë¡œìš´ í´ë˜ìŠ¤ ë°ì´í„° | í…ŒìŠ¤íŠ¸ ì‹œ | ì˜ˆì‹œ |
| --- | --- | --- | --- |
| **Zero-Shot** | ì—†ìŒ | Unseenë§Œ | â€œì¤„ë¬´ëŠ¬ ë§â€ ì„¤ëª… ë“£ê³  ì–¼ë£©ë§ ë§íˆê¸° |
| **One-Shot** | 1ê°œ | ê·¸ í´ë˜ìŠ¤ ì˜ˆì¸¡ | ì‚¬ì§„ 1ì¥ ë³´ê³  ì¸ë¬¼ ì¸ì‹ |
| **Few-Shot** | ì†ŒëŸ‰(2~ìˆ˜ì‹­ ê°œ) | ê·¸ í´ë˜ìŠ¤ ì˜ˆì¸¡ | 5ì¥ ë³´ê³  í’ˆì¢… ë¶„ë¥˜ |
| **GZSL** | ì—†ìŒ | Seen+Unseen ëª¨ë‘ | ê³ ì–‘ì´Â·ê°œÂ·ì–¼ë£©ë§ ëª¨ë‘ ë¶„ë¥˜ |

---

ğŸ“Œ **ì•”ê¸° íŒ**

- **ë°ì´í„° ì–‘**: Zero < One < Few
- **GZSL**: ZSL í™•ì¥íŒ + Seen í¬í•¨ â†’ bias ì¡°ì • í•„ìš”
- **One-Shot/Few-Shot**: ìƒˆë¡œìš´ í´ë˜ìŠ¤ì— ì ì€ ë°ì´í„° ì œê³µ
- **Zero-Shot**: ìƒˆë¡œìš´ í´ë˜ìŠ¤ì— ë°ì´í„° ìì²´ ì—†ìŒ, ëŒ€ì‹  ì„¤ëª…/ì†ì„± ì‚¬ìš©

### ë¬¸í•­ 21

# 1. NAS: AIê°€ AIë¥¼ ì„¤ê³„í•˜ë‹¤

NASëŠ” ì‰½ê²Œ ë§í•´ 'AIê°€ AIë¥¼ ì„¤ê³„í•˜ëŠ” ê¸°ìˆ 'ì…ë‹ˆë‹¤. ì§€ê¸ˆê¹Œì§€ AI ëª¨ë¸ì˜ êµ¬ì¡°(ì•„í‚¤í…ì²˜)ë¥¼ ì„¤ê³„í•˜ëŠ” ê²ƒì€ ì „ë¬¸ê°€ë“¤ì˜ ì˜ì—­ì´ì—ˆìŠµë‹ˆë‹¤. ì´ë“¤ì€ ë§ˆì¹˜ ê±´ì¶•ê°€ê°€ ê±´ë¬¼ì„ ì„¤ê³„í•˜ë“¯ AI ëª¨ë¸ì˜ ê° ì¸µê³¼ ì—°ê²° êµ¬ì¡°ë¥¼ ì„¸ì‹¬í•˜ê²Œ ê³„íší–ˆì£ . í•˜ì§€ë§Œ NASëŠ” ì´ ê³¼ì •ì„ ìë™í™”í•©ë‹ˆë‹¤.

ì˜ˆë¥¼ ë“¤ì–´, ì´ë¯¸ì§€ ì¸ì‹ì„ ìœ„í•œ AI ëª¨ë¸ì„ ë§Œë“ ë‹¤ê³  ìƒê°í•´ë´…ì‹œë‹¤. ì „ë¬¸ê°€ë¼ë©´ ìˆ˜ì‘ì—…ìœ¼ë¡œ ì—¬ëŸ¬ ì¸µì˜ ì‹ ê²½ë§ì„ ìŒ“ê³ , í•„í„° ìˆ˜, ì»¤ë„ í¬ê¸° ë“± ê° ì¸µì˜ íŠ¹ì„±ì„ ê²°ì •í•´ì•¼ í•©ë‹ˆë‹¤. ë°˜ë©´ NASëŠ” ì´ ëª¨ë“  ê³¼ì •ì„ ìë™ìœ¼ë¡œ ìˆ˜í–‰í•©ë‹ˆë‹¤. AIê°€ ìŠ¤ìŠ¤ë¡œ ìµœì ì˜ ì‹ ê²½ë§ êµ¬ì¡°ë¥¼ ì°¾ì•„ë‚´ëŠ” ê²ƒì´ì£ .

# 2. NASì˜ ì‘ë™ ì›ë¦¬: 3ë‹¨ê³„ í”„ë¡œì„¸ìŠ¤

NASì˜ ì‘ë™ ì›ë¦¬ëŠ” í¬ê²Œ ì„¸ ë‹¨ê³„ë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆìŠµë‹ˆë‹¤:

# 1) íƒìƒ‰ ê³µê°„ ì •ì˜

íƒìƒ‰ ê³µê°„ ì •ì˜ ë‹¨ê³„ì—ì„œëŠ” AI ëª¨ë¸ì— ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ëª¨ë“  êµ¬ì„± ìš”ì†Œ(ì˜ˆ: í•©ì„±ê³± ì¸µ, í’€ë§ ì¸µ, ì™„ì „ ì—°ê²° ì¸µ ë“±)ì™€ ê·¸ë“¤ ê°„ì˜ ê°€ëŠ¥í•œ ëª¨ë“  ì¡°í•©ì„ ì •ì˜í•©ë‹ˆë‹¤. ì´ëŠ” NASê°€ íƒìƒ‰í•  'ê°€ëŠ¥ì„±ì˜ ìš°ì£¼'ë¥¼ ë§Œë“œëŠ” ê³¼ì •ì´ë¼ê³  ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

# 2) íƒìƒ‰ ì „ëµ

ë‹¤ìŒ ë‹¨ê³„ì—ì„œëŠ” ì •ì˜ëœ íƒìƒ‰ ê³µê°„ì—ì„œ ìµœì ì˜ êµ¬ì¡°ë¥¼ ì°¾ëŠ” ë°©ë²•ì„ ê²°ì •í•©ë‹ˆë‹¤. ì£¼ë¡œ ì‚¬ìš©ë˜ëŠ” ë°©ë²•ìœ¼ë¡œëŠ” ê°•í™”í•™ìŠµ, ì§„í™” ì•Œê³ ë¦¬ì¦˜, ê·¸ë¼ë””ì–¸íŠ¸ ê¸°ë°˜ ìµœì í™” ë“±ì´ ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ê°•í™”í•™ìŠµì„ ì‚¬ìš©í•œë‹¤ë©´ AIëŠ” ëª¨ë¸ êµ¬ì¡°ë¥¼ í•˜ë‚˜ì”© 'ì‹œë„'í•´ë³´ê³ , ê·¸ ê²°ê³¼ì— ë”°ë¼ ë³´ìƒì„ ë°›ìŠµë‹ˆë‹¤. ì¢‹ì€ ì„±ëŠ¥ì„ ë‚´ëŠ” êµ¬ì¡°ì— ëŒ€í•´ì„œëŠ” ë†’ì€ ë³´ìƒì„ ë°›ê³ , ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë” ë‚˜ì€ êµ¬ì¡°ë¥¼ íƒìƒ‰í•´ ë‚˜ê°‘ë‹ˆë‹¤.

# 3) ì„±ëŠ¥ ì¶”ì • ì „ëµ

ì°¾ì•„ë‚¸ êµ¬ì¡°ì˜ ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤. ëª¨ë“  êµ¬ì¡°ë¥¼ ì™„ì „íˆ í•™ìŠµì‹œí‚¤ê³  í‰ê°€í•˜ëŠ” ê²ƒì€ ì‹œê°„ì´ ë„ˆë¬´ ì˜¤ë˜ ê±¸ë¦¬ê¸° ë•Œë¬¸ì—, ì£¼ë¡œ ë¶€ë¶„ í•™ìŠµì´ë‚˜ íŒŒë¼ë¯¸í„° ê³µìœ  ë“±ì˜ ê¸°ë²•ì„ ì‚¬ìš©í•˜ì—¬ ë¹ ë¥´ê²Œ ì„±ëŠ¥ì„ ì¶”ì •í•©ë‹ˆë‹¤.

# 3. NASì˜ ì¥ì ê³¼ ì°¨ë³„ì 

# 1) íš¨ìœ¨ì„±ê³¼ ê·œëª¨

NASëŠ” ì¸ê°„ ì „ë¬¸ê°€ë³´ë‹¤ í›¨ì”¬ ë” ë§ì€ êµ¬ì¡°ë¥¼ ë¹ ë¥´ê²Œ íƒìƒ‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 24ì‹œê°„ ì‰¬ì§€ ì•Šê³  ì¼í•˜ëŠ” AI ì„¤ê³„ìë¼ê³  ìƒê°í•´ë³´ì„¸ìš”. ì¸ê°„ì´ ëª‡ ë‹¬ í˜¹ì€ ëª‡ ë…„ì— ê±¸ì³ ì‹œë„í•´ë³¼ ìˆ˜ ìˆëŠ” ê²ƒë³´ë‹¤ í›¨ì”¬ ë” ë§ì€ ê°€ëŠ¥ì„±ì„ ë‹¨ì‹œê°„ ë‚´ì— íƒìƒ‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

# 2) ìƒˆë¡œìš´ ë°œê²¬ê³¼ í˜ì‹ 

ì¸ê°„ì˜ í¸ê²¬ì´ë‚˜ ê¸°ì¡´ ê´€í–‰ì— êµ¬ì• ë°›ì§€ ì•Šê¸° ë•Œë¬¸ì—, NASëŠ” ì¸ê°„ì´ ë¯¸ì²˜ ìƒê°í•˜ì§€ ëª»í•œ í˜ì‹ ì ì¸ êµ¬ì¡°ë¥¼ ë°œê²¬í•  ê°€ëŠ¥ì„±ì´ ìˆìŠµë‹ˆë‹¤. ì‹¤ì œë¡œ NASë¥¼ í†µí•´ ë°œê²¬ëœ ëª‡ëª‡ êµ¬ì¡°ë“¤ì€ ì¸ê°„ ì „ë¬¸ê°€ë“¤ì´ ë†€ë„ ë§Œí•œ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ì—ˆìŠµë‹ˆë‹¤.

# 3) ë³µì¡ì„± ê´€ë¦¬

í˜„ëŒ€ì˜ AI ëª¨ë¸ì€ ë§¤ìš° ë³µì¡í•˜ì—¬ ìˆ˜ë°±, ìˆ˜ì²œ ê°œì˜ ì¸µê³¼ ìˆ˜ë°±ë§Œ ê°œì˜ íŒŒë¼ë¯¸í„°ë¥¼ ê°€ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤. NASëŠ” ì´ëŸ¬í•œ ë³µì¡ì„±ì„ íš¨ê³¼ì ìœ¼ë¡œ ê´€ë¦¬í•˜ê³ , ì¸ê°„ì´ ì§ê´€ì ìœ¼ë¡œ íŒŒì•…í•˜ê¸° ì–´ë ¤ìš´ ë³µì¡í•œ ìƒí˜¸ì‘ìš©ì„ ê³ ë ¤í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

# 4) ë™ì  ì ì‘ì„±

NASëŠ” ìƒˆë¡œìš´ ë°ì´í„°ë‚˜ ìš”êµ¬ì‚¬í•­ì— ë”°ë¼ ëª¨ë¸ êµ¬ì¡°ë¥¼ ë™ì ìœ¼ë¡œ ì¡°ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŠ” ë³€í™”í•˜ëŠ” í™˜ê²½ì´ë‚˜ ë°ì´í„° íŠ¹ì„±ì— ë¹ ë¥´ê²Œ ëŒ€ì‘í•  ìˆ˜ ìˆê²Œ í•´ì¤ë‹ˆë‹¤.

# 5) ë‹¤ì¤‘ ëª©í‘œ ìµœì í™”

NASëŠ” ì •í™•ë„, ì†ë„, ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë“± ì—¬ëŸ¬ ëª©í‘œë¥¼ ë™ì‹œì— ê³ ë ¤í•˜ì—¬ ìµœì í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŠ” ì¸ê°„ì´ ìˆ˜ë™ìœ¼ë¡œ í•˜ê¸°ì—ëŠ” ë§¤ìš° ë³µì¡í•œ ì‘ì—…ì…ë‹ˆë‹¤.

# 6) ìë™í™”ì™€ ì ‘ê·¼ì„±

NASëŠ” AI ëª¨ë¸ ì„¤ê³„ ê³¼ì •ì„ ìë™í™”í•¨ìœ¼ë¡œì¨, AI ì „ë¬¸ê°€ê°€ ì•„ë‹Œ ì‚¬ëŒë“¤ë„ ê³ ì„±ëŠ¥ AI ëª¨ë¸ì„ ê°œë°œí•  ìˆ˜ ìˆê²Œ í•´ì¤ë‹ˆë‹¤. ì´ëŠ” AI ê¸°ìˆ ì˜ ë¯¼ì£¼í™”ì™€ ì ‘ê·¼ì„± í–¥ìƒì— ê¸°ì—¬í•©ë‹ˆë‹¤.

# 4. NASì˜ ë„ì „ê³¼ì œ: ì•ìœ¼ë¡œ í•´ê²°í•´ì•¼ í•  ë¬¸ì œë“¤

# 1) ê³„ì‚° íš¨ìœ¨ì„±ê³¼ í™•ì¥ì„±

NASì˜ ê°€ì¥ í° ë„ì „ ì¤‘ í•˜ë‚˜ëŠ” ì—„ì²­ë‚œ ê³„ì‚° ë¹„ìš©ì…ë‹ˆë‹¤. ìµœì‹  ì—°êµ¬ì—ì„œëŠ” ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë‹¤ì–‘í•œ ì ‘ê·¼ë²•ì´ ì‹œë„ë˜ê³  ìˆìŠµë‹ˆë‹¤. ì§„í™”ì  ì•Œê³ ë¦¬ì¦˜ê³¼ ë² ì´ì§€ì•ˆ ìµœì í™”ë¥¼ ê²°í•©í•œ í•˜ì´ë¸Œë¦¬ë“œ ë°©ì‹, ì‹ ê²½ë§ ê°€ì§€ì¹˜ê¸°ì™€ ì–‘ìí™” ê¸°ë²•ì„ NAS ê³¼ì •ì— í†µí•©í•˜ëŠ” ë°©ë²•, ê·¸ë¦¬ê³  ë©”íƒ€ëŸ¬ë‹ì„ í™œìš©í•œ íš¨ìœ¨ì ì¸ ì•„í‚¤í…ì²˜ íƒìƒ‰ ë“±ì´ ê·¸ ì˜ˆì…ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì´ëŸ¬í•œ ë°©ë²•ë“¤ë„ ëŒ€ê·œëª¨ ë¬¸ì œì— ì ìš©í•  ë•Œ ì—¬ì „íˆ í•œê³„ê°€ ìˆì–´ ê³„ì‚° íš¨ìœ¨ì„±ê³¼ í™•ì¥ì„± ê°œì„ ì€ ì§€ì†ì ì¸ ì—°êµ¬ ê³¼ì œì…ë‹ˆë‹¤.

# 2) ë‹¤ì¤‘ ëª©í‘œ ìµœì í™”ì˜ ë³µì¡ì„±

ì§€ì—° ì‹œê°„, ì—ë„ˆì§€ íš¨ìœ¨ì„±, ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë“± ì—¬ëŸ¬ ëª©í‘œë¥¼ ë™ì‹œì— ê³ ë ¤í•˜ëŠ” ë‹¤ì¤‘ ëª©í‘œ ìµœì í™”ëŠ” ë§¤ìš° ë³µì¡í•œ ë¬¸ì œì…ë‹ˆë‹¤. íŒŒë ˆí†  ìµœì  í•´ì§‘í•© íƒìƒ‰ì˜ ì–´ë ¤ì›€, ëª©í‘œ ê°„ íŠ¸ë ˆì´ë“œì˜¤í”„ë¥¼ ì •ëŸ‰í™”í•˜ëŠ” ë°©ë²•, ë™ì ìœ¼ë¡œ ë³€í™”í•˜ëŠ” í™˜ê²½ì—ì„œì˜ ë‹¤ì¤‘ ëª©í‘œ ìµœì í™” ë“±ì´ ì£¼ìš” ê³¼ì œë¡œ ë‚¨ì•„ ìˆìŠµë‹ˆë‹¤.

# 3) íƒìƒ‰ ê³µê°„ì˜ í‘œí˜„ë ¥ê³¼ íš¨ìœ¨ì„±

íš¨ê³¼ì ì¸ íƒìƒ‰ ê³µê°„ ì„¤ê³„ëŠ” NASì˜ ì„±ëŠ¥ì„ ì¢Œìš°í•˜ëŠ” í•µì‹¬ ìš”ì†Œì…ë‹ˆë‹¤. ì¶©ë¶„í•œ í‘œí˜„ë ¥ê³¼ íƒìƒ‰ íš¨ìœ¨ì„± ì‚¬ì´ì˜ ê· í˜•, ë„ë©”ì¸ ì§€ì‹ì„ íƒìƒ‰ ê³µê°„ì— íš¨ê³¼ì ìœ¼ë¡œ í†µí•©í•˜ëŠ” ë°©ë²•, ìƒˆë¡œìš´ ì—°ì‚°ìë‚˜ êµ¬ì¡°ë¥¼ ë™ì ìœ¼ë¡œ íƒìƒ‰ ê³µê°„ì— ì¶”ê°€í•˜ëŠ” ë°©ë²• ë“±ì´ ì¤‘ìš”í•œ ì—°êµ¬ ì£¼ì œì…ë‹ˆë‹¤.

# 4) ì„¤ëª… ê°€ëŠ¥ì„±ê³¼ í•´ì„ ê°€ëŠ¥ì„±

NASë¡œ ìƒì„±ëœ ëª¨ë¸ì˜ êµ¬ì¡°ì™€ ë™ì‘ ì›ë¦¬ë¥¼ ì´í•´í•˜ê³  ì„¤ëª…í•˜ëŠ” ê²ƒì€ ì¤‘ìš”í•œ ê³¼ì œì…ë‹ˆë‹¤. ë³µì¡í•œ NAS ëª¨ë¸ì˜ ì˜ì‚¬ê²°ì • ê³¼ì • í•´ì„, NAS ê³¼ì • ìì²´ì˜ íˆ¬ëª…ì„± í™•ë³´, ê·œì œ ë° ìœ¤ë¦¬ì  ì¸¡ë©´ì—ì„œì˜ ì„¤ëª… ê°€ëŠ¥ì„± ìš”êµ¬ ì¶©ì¡± ë“±ì´ ì´ì— í•´ë‹¹í•©ë‹ˆë‹¤.

# 5) ì „ì´ í•™ìŠµê³¼ ì¼ë°˜í™” ëŠ¥ë ¥

íŠ¹ì • íƒœìŠ¤í¬ë‚˜ ë°ì´í„°ì…‹ì— ìµœì í™”ëœ NAS ëª¨ë¸ì˜ ì¼ë°˜í™” ëŠ¥ë ¥ í–¥ìƒì€ ì¤‘ìš”í•œ ì—°êµ¬ ì£¼ì œì…ë‹ˆë‹¤. ë‹¤ì–‘í•œ ë„ë©”ì¸ ê°„ ì§€ì‹ ì „ì´ ë©”ì»¤ë‹ˆì¦˜ ê°œë°œ, ì œë¡œìƒ· ë˜ëŠ” í“¨ìƒ· í•™ìŠµ ëŠ¥ë ¥ì„ ê°–ì¶˜ NAS ëª¨ë¸ ì„¤ê³„, ì§€ì†ì  í•™ìŠµì„ ì§€ì›í•˜ëŠ” NAS í”„ë ˆì„ì›Œí¬ ê°œë°œ ë“±ì´ ì´ì— í¬í•¨ë©ë‹ˆë‹¤.

# 6) í¸í–¥ì„±ê³¼ ê³µì •ì„± ë¬¸ì œ

NAS ê³¼ì •ì—ì„œ ë°œìƒí•  ìˆ˜ ìˆëŠ” ì•Œê³ ë¦¬ì¦˜ í¸í–¥ì„±ê³¼ ë¶ˆê³µì •ì„± ë¬¸ì œë„ ì¤‘ìš”í•œ ë„ì „ ê³¼ì œì…ë‹ˆë‹¤. í›ˆë ¨ ë°ì´í„°ì˜ í¸í–¥ì´ NAS ê²°ê³¼ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ ë¶„ì„, ê³µì •ì„±ì„ ëª…ì‹œì  ëª©í‘œë¡œ í•˜ëŠ” NAS ì•Œê³ ë¦¬ì¦˜ ê°œë°œ, ë‹¤ì–‘í•œ ì¸êµ¬ í†µê³„í•™ì  ê·¸ë£¹ì— ëŒ€í•´ ê³µí‰í•œ ì„±ëŠ¥ì„ ë³´ì´ëŠ” ëª¨ë¸ ì„¤ê³„ ë“±ì´ ì´ì— í•´ë‹¹í•©ë‹ˆë‹¤.

**DARTS (Differentiable Architecture Search)**

## 1. ê°œë…

- *NAS(Neural Architecture Search)**ì˜ í•œ ë°©ë²•
- ê¸°ì¡´ NASëŠ” ì•„í‚¤í…ì²˜ íƒìƒ‰ì„ **ì´ì‚°(discrete)** ê³µê°„ì—ì„œ ìˆ˜í–‰ â†’ **ë¯¸ë¶„ ë¶ˆê°€ëŠ¥** â†’ ê°•í™”í•™ìŠµ(RL)ì´ë‚˜ ì§„í™” ì•Œê³ ë¦¬ì¦˜ì²˜ëŸ¼ ê³„ì‚°ëŸ‰ì´ í° ë°©ë²• ì‚¬ìš©
- **DARTS**ëŠ” ì´ì‚° ì„ íƒì„ **ì—°ì† ê³µê°„**ìœ¼ë¡œ ì™„í™”(continuous relaxation)í•˜ì—¬ **ê²½ì‚¬ í•˜ê°•ë²•(gradient descent)**ìœ¼ë¡œ ìµœì í™” ê°€ëŠ¥í•˜ê²Œ ë§Œë“  ë°©ë²•
- ë…¼ë¬¸: *Liu et al., 2018, "DARTS: Differentiable Architecture Search"*

---

## 2. í•µì‹¬ ì•„ì´ë””ì–´

1. **Search Space ì—°ì†í™”**
    - ê° ì—°ì‚°(ì˜ˆ: 3Ã—3 conv, 5Ã—5 conv, skip connection ë“±)ì„ **softmaxë¡œ ê°€ì¤‘ì¹˜ í•©** í˜•íƒœë¡œ í‘œí˜„
    - ì˜ˆ:o(i,j)(x)=kâˆ‘âˆ‘kâ€²exp(Î±kâ€²(i,j))exp(Î±k(i,j))â‹…opk(x)
        
        o(i,j)(x)=âˆ‘kexpâ¡(Î±k(i,j))âˆ‘kâ€²expâ¡(Î±kâ€²(i,j))â‹…opk(x)o^{(i,j)}(x) = \sum_{k} \frac{\exp(\alpha_k^{(i,j)})}{\sum_{k'}\exp(\alpha_{k'}^{(i,j)})} \cdot op_k(x)
        
        ì—¬ê¸°ì„œ Î±\alphaÎ±ê°€ í•™ìŠµ ê°€ëŠ¥í•œ ì•„í‚¤í…ì²˜ íŒŒë¼ë¯¸í„°
        
2. **2ë‹¨ê³„ ìµœì í™”(Bi-level optimization)**
    - **ëª¨ë¸ íŒŒë¼ë¯¸í„° www**: í›ˆë ¨ ë°ì´í„°ë¡œ í•™ìŠµ
    - **ì•„í‚¤í…ì²˜ íŒŒë¼ë¯¸í„° Î±\alphaÎ±**: ê²€ì¦ ë°ì´í„°ë¡œ í•™ìŠµ
3. íƒìƒ‰ì´ ëë‚˜ë©´, softmax ê°€ì¤‘ì¹˜ì—ì„œ ê°€ì¥ í° ì—°ì‚°ì„ ì„ íƒí•´ **ì´ì‚° ì•„í‚¤í…ì²˜**ë¡œ ë³€í™˜

---

## 3. ì¥ì 

- **ê³„ì‚° íš¨ìœ¨ì„±**: RL/ì§„í™” ê¸°ë°˜ NASë³´ë‹¤ í›¨ì”¬ ë¹ ë¦„
- **ë¯¸ë¶„ ê°€ëŠ¥ì„±**: í‘œì¤€ ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬(PyTorch, TensorFlow)ì—ì„œ ì‰½ê²Œ êµ¬í˜„ ê°€ëŠ¥
- **ê²½í—˜ì  ì„±ëŠ¥**: ì´ë¯¸ì§€ ë¶„ë¥˜(CIFAR-10, ImageNet) ë“±ì—ì„œ ì¢‹ì€ ì„±ëŠ¥ ë‹¬ì„±

---

## 4. í•œê³„

- **ì—°ì† ì™„í™”ë¡œ ì¸í•œ bias**: ì‹¤ì œ discrete ì„ íƒê³¼ í•™ìŠµ ì¤‘ softmax í˜¼í•©ì´ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ
- local minima ë¬¸ì œ ë°œìƒ ê°€ëŠ¥
- memory ì‚¬ìš©ëŸ‰ ì¦ê°€(ëª¨ë“  í›„ë³´ ì—°ì‚°ì„ ë™ì‹œì— ê³„ì‚°)

---

## 5. ì‘ìš©

- ì´ë¯¸ì§€ ë¶„ë¥˜, ì‹œê³„ì—´ ë¶„ì„, NLP ëª¨ë¸ ì•„í‚¤í…ì²˜ íƒìƒ‰
- ë‹¤ì–‘í•œ ë³€í˜• ê¸°ë²• ì¡´ì¬(PC-DARTS, Fair-DARTS, RobustDARTS ë“±)

---

ğŸ“Œ **ì•”ê¸° í¬ì¸íŠ¸**

> â€œDARTS = NASë¥¼ ë¯¸ë¶„ ê°€ëŠ¥í•˜ê²Œ ë§Œë“  ë°©ë²•, discrete â†’ continuous, gradient descentë¡œ ë¹ ë¥¸ íƒìƒ‰â€
> 

**Q-learning**

## 1. ê°œë…

- *ê°•í™”í•™ìŠµ(RL)**ì˜ ëŒ€í‘œì ì¸ **ê°’ ê¸°ë°˜(Value-based)** ì•Œê³ ë¦¬ì¦˜
- â€œì–´ë–¤ ìƒíƒœì—ì„œ ì–´ë–¤ í–‰ë™ì„ í–ˆì„ ë•Œ ì–»ì„ ìˆ˜ ìˆëŠ” **ì¥ê¸°ì ì¸ ë³´ìƒ**â€ì„ **Qê°’(Q-value)**ìœ¼ë¡œ í‘œí˜„
- Qê°’ì„ í…Œì´ë¸” í˜•íƒœë¡œ ì €ì¥í•˜ê³  ì—…ë°ì´íŠ¸í•˜ë©´ì„œ **ìµœì  ì •ì±…**(Optimal Policy)ì„ í•™ìŠµ

---

## 2. í•µì‹¬ ì•„ì´ë””ì–´

- **Q(s, a)**: ìƒíƒœ sssì—ì„œ í–‰ë™ aaaë¥¼ í–ˆì„ ë•Œ ì–»ëŠ” **ëˆ„ì  ë³´ìƒ ê¸°ëŒ€ê°’**
- **ëª©í‘œ**: Qê°’ì„ ì ì  ë” ì •í™•í•˜ê²Œ ì¶”ì •í•´, ë§¤ ìƒí™©ì—ì„œ Qê°’ì´ ê°€ì¥ í° í–‰ë™ ì„ íƒ
- **ì—…ë°ì´íŠ¸ ê·œì¹™** (ë²¨ë§Œ ë°©ì •ì‹ ê¸°ë°˜):

Q(s,a)â†Q(s,a)+Î±[r+Î³maxâ¡aâ€²Q(sâ€²,aâ€²)âˆ’Q(s,a)]Q(s,a) \leftarrow Q(s,a) + \alpha \big[ r + \gamma \max_{a'} Q(s',a') - Q(s,a) \big]

Q(s,a)â†Q(s,a)+Î±[r+Î³aâ€²maxQ(sâ€²,aâ€²)âˆ’Q(s,a)]

- Î±\alphaÎ±: í•™ìŠµë¥ 
- rrr: í˜„ì¬ í–‰ë™ìœ¼ë¡œ ë°›ì€ ë³´ìƒ
- Î³\gammaÎ³: ë¯¸ë˜ ë³´ìƒ í• ì¸ìœ¨
- sâ€²s'sâ€²: ë‹¤ìŒ ìƒíƒœ
- aâ€²a'aâ€²: ë‹¤ìŒ ìƒíƒœì—ì„œ ê°€ëŠ¥í•œ í–‰ë™ë“¤

---

## 3. ë™ì‘ ì ˆì°¨

1. **Q í…Œì´ë¸” ì´ˆê¸°í™”** (ëª¨ë“  ìƒíƒœ-í–‰ë™ ìŒì˜ Qê°’ì„ 0ìœ¼ë¡œ ì‹œì‘)
2. **ì—ì´ì „íŠ¸ê°€ í™˜ê²½ì—ì„œ í–‰ë™** ì„ íƒ (íƒí—˜ vs í™œìš©, e.g. Îµ-greedy ì •ì±…)
3. **ë³´ìƒ rê³¼ ë‹¤ìŒ ìƒíƒœ sâ€² ê´€ì°°**
4. **Qê°’ ì—…ë°ì´íŠ¸** (ìœ„ ìˆ˜ì‹ ì‚¬ìš©)
5. ëª©í‘œ: Qê°’ì´ ìˆ˜ë ´í•˜ë©´, ê° ìƒíƒœì—ì„œ Qê°’ì´ ìµœëŒ€ì¸ í–‰ë™ì„ ì„ íƒ â†’ ìµœì  ì •ì±… ì™„ì„±

---

## 4. íŠ¹ì§•

- **ëª¨ë¸ í”„ë¦¬(model-free)**: í™˜ê²½ì˜ ì „ì´ í™•ë¥ , ë³´ìƒ ë¶„í¬ë¥¼ ëª°ë¼ë„ í•™ìŠµ ê°€ëŠ¥
- ìƒíƒœÂ·í–‰ë™ ê³µê°„ì´ **ì‘ì„ ë•Œ** íš¨ìœ¨ì 
- **ë‹¨ì **: ìƒíƒœ/í–‰ë™ì´ ë§ê±°ë‚˜ ì—°ì† ê³µê°„ì´ë©´ Q í…Œì´ë¸”ì´ ë„ˆë¬´ ì»¤ì§ â†’ DQN ê°™ì€ ì‹ ê²½ë§ ê¸°ë°˜ìœ¼ë¡œ í™•ì¥ í•„ìš”

---

## 5. ê°„ë‹¨ ì˜ˆì‹œ

**ë¯¸ë¡œ íƒˆì¶œ ê²Œì„**

- ìƒíƒœ s: ì—ì´ì „íŠ¸ ìœ„ì¹˜
- í–‰ë™ a: ìƒ, í•˜, ì¢Œ, ìš°
- ë³´ìƒ r: ì¶œêµ¬ì— ë„ë‹¬í•˜ë©´ +10, ë²½ ë¶€ë”ªíˆë©´ -1
- Q-learningì€ ì—¬ëŸ¬ ì—í”¼ì†Œë“œë¥¼ ê±°ì¹˜ë©° ê° ìœ„ì¹˜-í–‰ë™ì˜ Qê°’ì„ ì—…ë°ì´íŠ¸í•´, ê²°êµ­ ì¶œêµ¬ë¡œ ê°€ëŠ” ìµœì  ê²½ë¡œë¥¼ í•™ìŠµ

---

ğŸ“Œ **ì•”ê¸° í¬ì¸íŠ¸**

> Q-Learning = â€œìƒíƒœ-í–‰ë™ ê°€ì¹˜(Q)ë¥¼ í…Œì´ë¸”ì— ì €ì¥í•˜ê³ , ë²¨ë§Œ ë°©ì •ì‹ìœ¼ë¡œ ì—…ë°ì´íŠ¸í•˜ëŠ” ê°’ ê¸°ë°˜ RLâ€
> 

**Deep Q-Network**

## 1. ê°œë…

- **Deep Q-Learning**ì˜ í•œ í˜•íƒœ
- Q-Learning(ê°’ ê¸°ë°˜ RL)ì˜ Q-í•¨ìˆ˜ë¥¼ **ì‹ ê²½ë§**ìœ¼ë¡œ ê·¼ì‚¬í•˜ëŠ” ë°©ë²•
- Google DeepMindê°€ 2015ë…„ ë°œí‘œ (*Mnih et al., Nature 2015*)
- Atari ê²Œì„ ë“±ì—ì„œ **í”½ì…€ ì…ë ¥ë§Œìœ¼ë¡œ ì¸ê°„ ìˆ˜ì¤€ ì„±ëŠ¥** ë‹¬ì„±

---

## 2. í•µì‹¬ ì•„ì´ë””ì–´

1. **Q-Learning** ë³µìŠµ
    - Q(s, a): ìƒíƒœ sì—ì„œ í–‰ë™ aë¥¼ í–ˆì„ ë•Œì˜ ì¥ê¸°ì  ë³´ìƒ ê¸°ëŒ€ê°’
    - ë²¨ë§Œ ë°©ì •ì‹(Bellman Equation) ê¸°ë°˜ ì—…ë°ì´íŠ¸:Q(s,a)â†Q(s,a)+Î±[r+Î³aâ€²maxQ(sâ€²,aâ€²)âˆ’Q(s,a)]
        
        Q(s,a)â†Q(s,a)+Î±[r+Î³maxâ¡aâ€²Q(sâ€²,aâ€²)âˆ’Q(s,a)]Q(s,a) \leftarrow Q(s,a) + \alpha [r + \gamma \max_{a'} Q(s',a') - Q(s,a)]
        
2. **ì‹ ê²½ë§ í™œìš©**
    - Q(s, a)ë¥¼ ì§ì ‘ í…Œì´ë¸”ë¡œ ì €ì¥í•˜ì§€ ì•Šê³ , **ë”¥ëŸ¬ë‹ ëª¨ë¸**ì´ Qê°’ì„ ì˜ˆì¸¡
    - ì…ë ¥: ìƒíƒœ(ì˜ˆ: ì´ë¯¸ì§€ í”„ë ˆì„)
    - ì¶œë ¥: ê°€ëŠ¥í•œ í–‰ë™ë³„ Qê°’
3. **ì•ˆì •ì„± ê°œì„  ê¸°ë²•**
    - **ê²½í—˜ ì¬ìƒ(Experience Replay)**: í•™ìŠµ ìƒ˜í”Œì„ ë²„í¼ì— ì €ì¥í•˜ê³  ë¬´ì‘ìœ„ ì¶”ì¶œ â†’ ë°ì´í„° ìƒê´€ì„± ê°ì†Œ
    - **íƒ€ê²Ÿ ë„¤íŠ¸ì›Œí¬(Target Network)**: ì¼ì • ì£¼ê¸°ë§ˆë‹¤ Q-íƒ€ê²Ÿ ê³„ì‚°ìš© ë„¤íŠ¸ì›Œí¬ë¥¼ ê³ ì • â†’ í•™ìŠµ ì•ˆì •í™”

---

## 3. ì¥ì 

- ê³ ì°¨ì› ìƒíƒœ ê³µê°„(ì´ë¯¸ì§€, ì„¼ì„œ ë°ì´í„° ë“±) ì²˜ë¦¬ ê°€ëŠ¥
- ëª¨ë¸ì´ ì§ì ‘ íŠ¹ì§•(feature)ì„ í•™ìŠµí•˜ë¯€ë¡œ ìˆ˜ì‘ì—… ì„¤ê³„ ë¶ˆí•„ìš”
- ë‹¤ì–‘í•œ RL ë¬¸ì œ(ê²Œì„, ë¡œë³´í‹±ìŠ¤)ì— ì ìš© ê°€ëŠ¥

---

## 4. í•œê³„

- ìƒ˜í”Œ íš¨ìœ¨ ë‚®ìŒ(ë§ì€ ë°ì´í„° í•„ìš”)
- ë¶ˆì•ˆì •í•œ í•™ìŠµ ê°€ëŠ¥ì„± â†’ Double DQN, Dueling DQN, Prioritized Replay ë“± ê°œì„  ì•Œê³ ë¦¬ì¦˜ ë“±ì¥

---

## 5. ëŒ€í‘œ ì‘ìš©

- Atari 2600 ê²Œì„ í”Œë ˆì´
- ììœ¨ì£¼í–‰ ì‹œë®¬ë ˆì´ì…˜
- ë¡œë´‡ ì œì–´

---

ğŸ“Œ **ì•”ê¸° í¬ì¸íŠ¸**

> DQN = â€œQ-Learning + Deep Neural Network + Experience Replay + Target Networkâ€
> 

### ë¬¸í•­ 22

overfitting vs underfitting

## 1. ê°œë… ë¹„êµ

| êµ¬ë¶„ | Overfitting | Underfitting |
| --- | --- | --- |
| **ì •ì˜** | í•™ìŠµ ë°ì´í„°ì— ë„ˆë¬´ ê³¼ë„í•˜ê²Œ ë§ì¶°ì ¸ì„œ, ìƒˆë¡œìš´ ë°ì´í„°(í…ŒìŠ¤íŠ¸ì…‹)ì—ì„œ ì„±ëŠ¥ì´ ë–¨ì–´ì§€ëŠ” í˜„ìƒ | í•™ìŠµ ë°ì´í„°ì—ë„ ì¶©ë¶„íˆ ë§ì¶”ì§€ ëª»í•´, ì „ë°˜ì ìœ¼ë¡œ ì„±ëŠ¥ì´ ë‚®ì€ í˜„ìƒ |
| **ì›ì¸** | ëª¨ë¸ ë³µì¡ë„ ê³¼ë„, í•™ìŠµ ë°ì´í„°ì— ì¡´ì¬í•˜ëŠ” ë…¸ì´ì¦ˆê¹Œì§€ í•™ìŠµ | ëª¨ë¸ì´ ë„ˆë¬´ ë‹¨ìˆœí•˜ê±°ë‚˜, í•™ìŠµ ë¶€ì¡± |
| **í•™ìŠµ ê³¡ì„  íŠ¹ì§•** | í›ˆë ¨ ì •í™•ë„ â†‘, ê²€ì¦ ì •í™•ë„ â†“ | í›ˆë ¨ ì •í™•ë„ì™€ ê²€ì¦ ì •í™•ë„ ëª¨ë‘ ë‚®ìŒ |
| **ë¹„ìœ ** | ì‹œí—˜ ì˜ˆìƒë¬¸ì œë§Œ ë‹¬ë‹¬ ì™¸ì›€ | ê°œë… ìì²´ë¥¼ ì œëŒ€ë¡œ ì´í•´ ëª»í•¨ |

---

## 2. íŠ¹ì§•

### Overfitting

- í›ˆë ¨ ë°ì´í„° ì„±ëŠ¥: **ë§¤ìš° ë†’ìŒ**
- ê²€ì¦/í…ŒìŠ¤íŠ¸ ë°ì´í„° ì„±ëŠ¥: **ë‚®ìŒ**
- ì¼ë°˜í™” ì„±ëŠ¥ ì €í•˜
- ì£¼ë¡œ **ë³µì¡í•œ ëª¨ë¸**ì—ì„œ ë°œìƒ (ì‹¬ì¸µ ì‹ ê²½ë§, íŒŒë¼ë¯¸í„° ìˆ˜ ê³¼ë‹¤ ë“±)

### Underfitting

- í›ˆë ¨ ë°ì´í„° ì„±ëŠ¥: **ë‚®ìŒ**
- ê²€ì¦/í…ŒìŠ¤íŠ¸ ë°ì´í„° ì„±ëŠ¥: **ë” ë‚®ìŒ**
- í•™ìŠµì´ ëœ ë˜ì—ˆê±°ë‚˜ ëª¨ë¸ ìš©ëŸ‰ ë¶€ì¡±
- ì£¼ë¡œ **ë‹¨ìˆœí•œ ëª¨ë¸**ì—ì„œ ë°œìƒ (ì„ í˜• ëª¨ë¸ë¡œ ë³µì¡í•œ íŒ¨í„´ í•™ìŠµ ì‹œ ë“±)

---

## 3. í•´ê²° ë°©ë²•

### Overfitting ë°©ì§€

1. **ë°ì´í„° ì¸¡ë©´**
    - ë°ì´í„° ì–‘ ëŠ˜ë¦¬ê¸° (ìˆ˜ì§‘, ë°ì´í„° ì¦ê°•)
    - ë…¸ì´ì¦ˆ ì œê±°
2. **ëª¨ë¸ êµ¬ì¡° ì¸¡ë©´**
    - ëª¨ë¸ ë‹¨ìˆœí™” (ì¸µ ìˆ˜, íŒŒë¼ë¯¸í„° ìˆ˜ ì¶•ì†Œ)
    - Dropout, Batch Normalization ì ìš©
3. **í•™ìŠµ ê³¼ì • ì¸¡ë©´**
    - ì •ê·œí™”(Regularization) ì ìš©: L1, L2
    - Early Stopping
    - Cross-Validation í™œìš©
4. **ì•™ìƒë¸” ê¸°ë²•**
    - Bagging, Boosting, Stacking

### Underfitting ë°©ì§€

1. **ëª¨ë¸ ë³µì¡ë„ ì¦ê°€**
    - ë” ê¹Šì€ ì‹ ê²½ë§, íŒŒë¼ë¯¸í„° ìˆ˜ ì¦ê°€
2. **í•™ìŠµ ê³¼ì • ê°œì„ **
    - í•™ìŠµ ì‹œê°„(ì—í­) ëŠ˜ë¦¬ê¸°
    - í•™ìŠµë¥ (Learning Rate) ì¡°ì •
3. **íŠ¹ì§•(feature) ê°•í™”**
    - ë” êµ¬ì²´ì ì´ê³  ìœ ìš©í•œ íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§
4. **ì •ê·œí™” ì•½í™”**
    - L1/L2 ê³„ìˆ˜ ì¤„ì´ê¸°, Dropout ë¹„ìœ¨ ë‚®ì¶”ê¸°

---

ğŸ“Œ **ì•”ê¸° íŒ**

- Overfitting = "Too much memorizing" â†’ ëª¨ë¸ ë‹¨ìˆœí™” & ì •ê·œí™”
- Underfitting = "Too simple or too shallow" â†’ ëª¨ë¸ ë³µì¡í™” & í•™ìŠµ ê°•í™”

### ë¬¸í•­ 24

https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning

â†’ ê·¸ë¦¼ì´ ì´ê±°ë‘ ë˜‘ê°™ìŒ

## **MLOps Level 0: Manual Process â€“ Key Points**

### 1. **Characteristics**

1. **Fully manual, script-driven**
    - Data analysis â†’ preparation â†’ training â†’ validation are all manual.
    - Often done in notebooks with experimental code by data scientists.
2. **Disconnection between ML and Ops**
    - Data scientists hand off a trained model artifact to engineers for deployment.
    - Engineers prepare production features, risking **trainingâ€“serving skew**.
        - **Trainingâ€“serving skew** happens when the data or features your model sees **during training** are different from what it sees **in production (serving)**.
3. **Low release frequency**
    - Model changes/re-training only a few times per year.
4. **No CI/CD**
    - No Continuous Integration: testing happens only within notebooks/scripts.
    - No Continuous Deployment: model deployment is infrequent and manual.
5. **Deployment scope**
    - Focused on serving the model as a prediction service (e.g., REST API), not the entire ML system.
6. **No active performance monitoring**
    - No logging/tracking of predictions â†’ unable to detect drift or degradation.

---

### 2. **Operational Reality**

- Engineering teams still run their own complex API deployment pipeline (security, regression, load, canary testing).
- New models go through A/B testing or online experiments before full rollout.

---

### 3. **Challenges**

- Models degrade in real-world environments due to:
    - Changing data distributions
    - Evolving environment dynamics
- Lack of adaptation and maintenance â†’ model failures in production.

---

### 4. **Recommended Improvements**

1. **Active monitoring**
    - Track performance to detect degradation and drift.
2. **Frequent retraining**
    - Incorporate the latest data (e.g., fashion recommendation adapting to new trends).
3. **Continuous experimentation**
    - Improve features, architecture, hyperparameters.
4. **Introduce CI/CD and CT**
    - **CT (Continuous Training)**: automated re-training pipelines.
    - **CI/CD**: quickly test, build, and deploy new pipeline/model versions.

---

**Exam tip:**

Keywords = *Manual, Script-driven, No CI/CD, Low frequency deploys, No monitoring, Trainingâ€“Serving skew*

## **MLOps Level 1: ML Pipeline Automation â€“ Key Points**

### 1. **Goal**

- Automate the **ML training pipeline** to enable **Continuous Training (CT)**.
- Use fresh data to retrain and redeploy models automatically.

---

### 2. **Main Characteristics**

1. **Automated orchestration** of ML experiment steps â†’ faster iteration.
2. **CT in production**: Automatic retraining using live data triggers.
3. **Experimentalâ€“operational symmetry**: Same pipeline code for dev, pre-prod, and prod environments.
4. **Modularized, containerized components**:
    - Reusable, composable across pipelines.
    - Isolate runtime environments for reproducibility.
5. **Continuous delivery of models**:
    - Trained + validated models are automatically deployed as prediction services.
6. **Pipeline deployment**:
    - Deploy entire training pipeline (not just a model) to run recurrently.

---

### 3. **Additional Components**

- **Data validation**:
    - **Schema skew**: Missing/unexpected features â†’ stop pipeline.
    - **Value skew**: Significant statistical changes â†’ trigger retraining.
- **Model validation**:
    - Compare metrics with current/baseline models.
    - Check performance consistency across data segments.
    - Verify infrastructure compatibility & API consistency.
    - Canary/A/B testing before full rollout.
- **Feature Store (optional)**:
    - Centralized repository for training + serving features.
    - Prevent trainingâ€“serving skew.
- **Metadata management**:
    - Record pipeline/component versions, parameters, artifacts, metrics.
    - Enable reproducibility, rollback, and debugging.
- **Pipeline triggers**:
    - On demand, scheduled, new data arrival, performance drop, concept drift.

---

### 4. **Challenges**

- If pipeline implementations arenâ€™t updated often and only a few pipelines are managed, manual testing/deployment is still common.
- For frequent changes or many pipelines â†’ need **CI/CD for ML pipelines**.

---

**Exam keywords**:

*Automated CT pipeline, Data/Model validation, Experimentalâ€“operational symmetry, Modular components, Feature store, Metadata tracking, Pipeline triggers.*

## **MLOps Level 0 vs Level 1 ë¹„êµí‘œ**

| êµ¬ë¶„ | **Level 0: Manual Process** | **Level 1: ML Pipeline Automation** |
| --- | --- | --- |
| **í”„ë¡œì„¸ìŠ¤ ë°©ì‹** | ì „ ê³¼ì • ìˆ˜ë™, ìŠ¤í¬ë¦½íŠ¸Â·ë…¸íŠ¸ë¶ ê¸°ë°˜ | íŒŒì´í”„ë¼ì¸ ìë™í™”(Orchestration) |
| **ML â†” Ops ê´€ê³„** | ë°ì´í„° ì‚¬ì´ì–¸ìŠ¤ íŒ€ê³¼ ìš´ì˜íŒ€ ë¶„ë¦¬, ëª¨ë¸ ì•„í‹°íŒ©íŠ¸ ìˆ˜ë™ ì „ë‹¬ | ê°œë°œÂ·ìš´ì˜ í™˜ê²½ ë™ì¼ íŒŒì´í”„ë¼ì¸ ì‚¬ìš© (Experimentalâ€“Operational Symmetry) |
| **ì¬í•™ìŠµ(CT)** | ì—†ìŒ, ì—° 1~2íšŒ ìˆ˜ë™ ì¬í•™ìŠµ | ì‹¤ì‹œê°„ íŠ¸ë¦¬ê±° ê¸°ë°˜ ìë™ ì¬í•™ìŠµ (Continuous Training) |
| **ë°°í¬ ë‹¨ìœ„** | í•™ìŠµëœ ëª¨ë¸ë§Œ ë°°í¬ (ì˜ˆì¸¡ ì„œë¹„ìŠ¤) | ì „ì²´ í•™ìŠµ íŒŒì´í”„ë¼ì¸ ë°°í¬, ì£¼ê¸°ì  ì‹¤í–‰ |
| **CI/CD** | ì—†ìŒ | ì¼ë¶€ ìë™í™” ê°€ëŠ¥, CT ì¤‘ì‹¬, CI/CDë¡œ í™•ì¥ ê°€ëŠ¥ |
| **ë°ì´í„°Â·ëª¨ë¸ ê²€ì¦** | ìˆ˜ë™, ì œí•œì  | ìë™ ë°ì´í„° ê²€ì¦(ìŠ¤í‚¤ë§ˆÂ·ê°’ í¸í–¥), ëª¨ë¸ ê²€ì¦(ì„±ëŠ¥ ë¹„êµÂ·ì„¸ê·¸ë¨¼íŠ¸ë³„ ì¼ê´€ì„±Â·ë°°í¬ ì „ í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸) |
| **Feature Store** | ì—†ìŒ | ì„ íƒì ìœ¼ë¡œ ì‚¬ìš©, í•™ìŠµâ€“ì„œë¹™ ìŠ¤í ë°©ì§€ |
| **ë©”íƒ€ë°ì´í„° ê´€ë¦¬** | ì œí•œì , ìˆ˜ë™ ê¸°ë¡ | íŒŒì´í”„ë¼ì¸ ì‹¤í–‰Â·ì•„í‹°íŒ©íŠ¸Â·ëª¨ë¸ ë²„ì „Â·í‰ê°€ ì§€í‘œ ìë™ ê¸°ë¡ |
| **ëª¨ë‹ˆí„°ë§** | ê±°ì˜ ì—†ìŒ, ì˜ˆì¸¡Â·ë“œë¦¬í”„íŠ¸ ì¶”ì  ë¶€ì¬ | íŠ¸ë¦¬ê±° ì¡°ê±´ìœ¼ë¡œ ë°ì´í„° ë¶„í¬ ë³€í™”Â·ì„±ëŠ¥ ì €í•˜ ê°ì§€ ê°€ëŠ¥ |
| **ì¥ì ** | ê°„ë‹¨, ì´ˆê¸° ML ì ìš©ì— ì í•© | ì¬í˜„ì„±Â·ì†ë„Â·í™•ì¥ì„±â†‘, ìƒˆë¡œìš´ ë°ì´í„°ì— ë¹ ë¥¸ ëŒ€ì‘ |
| **í•œê³„** | ì¼ë°˜í™” ì„±ëŠ¥ ì €í•˜, ìš´ì˜ í™˜ê²½ ì ì‘ ì–´ë ¤ì›€ | ë§ì€ íŒŒì´í”„ë¼ì¸Â·ìƒˆ êµ¬í˜„ ë¹ˆë²ˆ ì‹œ CI/CD í•„ìš” |

---

ğŸ“Œ **ì‹œí—˜ ì•”ê¸° í¬ì¸íŠ¸**

- **Level 0** = *Manual, No CI/CD, Model-only deployment, No monitoring*
- **Level 1** = *Automated CT pipeline, Data/Model validation, Metadata tracking, Feature store option*

| êµ¬ë¶„ | Level 1 (CT ì¤‘ì‹¬) | CI/CD ì™„ì „ ìë™í™” (Level 2) |
| --- | --- | --- |
| **ì£¼ìš” ëª©ì ** | ìƒˆ ë°ì´í„°ì— ë”°ë¥¸ ìë™ ì¬í•™ìŠµ(CT) | ìƒˆ ì½”ë“œÂ·êµ¬í˜„ ë³€í™”ë„ ìë™ ë¹Œë“œÂ·í…ŒìŠ¤íŠ¸Â·ë°°í¬ |
| **ìë™í™” ë²”ìœ„** | ë°ì´í„°Â·ëª¨ë¸ ê²€ì¦, ëª¨ë¸ ë°°í¬ ìë™í™” | ë°ì´í„°Â·ëª¨ë¸ + íŒŒì´í”„ë¼ì¸ ì½”ë“œê¹Œì§€ ìë™í™” |
| **CI (Continuous Integration)** | ê±°ì˜ ì—†ìŒ ë˜ëŠ” ìˆ˜ë™ | ì½”ë“œ ë³€ê²½ ì‹œ ìë™ ë¹Œë“œÂ·í…ŒìŠ¤íŠ¸ |
| **CD (Continuous Delivery/Deployment)** | ëª¨ë¸ ë°°í¬ ì¼ë¶€ ìë™í™” | ì „ì²´ ML ì‹œìŠ¤í…œ(íŒŒì´í”„ë¼ì¸ í¬í•¨) ìë™ ë°°í¬ |
| **ëŒ€ìƒ ë³€ê²½ ìš”ì†Œ** | ë°ì´í„° ë³€í™” | ë°ì´í„° + ì½”ë“œ/êµ¬í˜„ ë³€í™” ëª¨ë‘ |
| **ì í•© ìƒí™©** | ë°ì´í„° ë³€í™” ì£¼ê¸°ê°€ ë¹ ë¥´ì§€ë§Œ ì½”ë“œ ë³€ê²½ì€ ë“œë¬¸ ê²½ìš° | ë°ì´í„°Â·ì½”ë“œ ë³€ê²½ ëª¨ë‘ ë¹ˆë²ˆí•œ ê²½ìš° |

ğŸ“Œ **ì •ë¦¬**

- **Level 1** â†’ "ë°ì´í„° ë³€í™”"ì— ë§ì¶° ëª¨ë¸ì„ **ìë™ ì¬í•™ìŠµ + ë°°í¬**í•˜ëŠ” ë‹¨ê³„.
- **Level 2** â†’ ë°ì´í„° ë³€í™”ë¿ ì•„ë‹ˆë¼ **ì½”ë“œÂ·êµ¬í˜„ ë³€ê²½**ê¹Œì§€ í¬í•¨í•´ **CI/CD ì „ë©´ ìë™í™”**.

## **MLOps Level 2 â€“ CI/CD Pipeline Automation**

**Goal:**

Enable rapid and reliable pipeline updates in production by fully automating build, test, and deployment with CI/CD, allowing fast experimentation and delivery of new ML ideas.

### **Key Characteristics**

- **Full Automation:** Combines Level 1â€™s automated ML pipeline (CT) with CI/CD for code and pipeline updates.
- **Stages:**
    1. **Development & Experimentation** â€“ Try new algorithms, architectures, features; push source code to repository.
    2. **Pipeline CI** â€“ Build, unit/integration test pipeline components (packages, containers, executables).
    3. **Pipeline CD** â€“ Deploy new pipeline implementation to target environment.
    4. **Automated Triggering** â€“ Run pipeline based on schedule or events.
    5. **Model CD** â€“ Deploy trained model as prediction service.
    6. **Monitoring** â€“ Track model performance, trigger retraining or new experiments.
- **Validation:** Data, model performance, API compatibility, and serving performance are automatically tested before deployment.
- **Deployment Strategy:** Gradual rollout (dev â†’ pre-prod â†’ prod) with automated and semi-automated steps.

### **Benefits**

- Rapid iteration and deployment of both data-driven changes and code-driven improvements.
- Reduced risk of production issues through automated testing.
- Faster adaptation to data drift, concept drift, and evolving business needs.

| êµ¬ë¶„ | **Level 0** â€“ Manual Process | **Level 1** â€“ ML Pipeline Automation | **Level 2** â€“ CI/CD Pipeline Automation |
| --- | --- | --- | --- |
| **í”„ë¡œì„¸ìŠ¤** | ì „ ê³¼ì • ìˆ˜ë™ ì‹¤í–‰ (ë°ì´í„° ì¤€ë¹„, í•™ìŠµ, ê²€ì¦, ë°°í¬) | íŒŒì´í”„ë¼ì¸ ë‹¨ê³„ ìë™í™”, ì§€ì†ì  í•™ìŠµ(CT) ê°€ëŠ¥ | íŒŒì´í”„ë¼ì¸ ìë™í™” + CI/CDë¡œ ì½”ë“œÂ·íŒŒì´í”„ë¼ì¸ ìë™ í…ŒìŠ¤íŠ¸Â·ë°°í¬ |
| **ìë™í™” ë²”ìœ„** | ì—†ìŒ | ë°ì´í„° ì²˜ë¦¬Â·ëª¨ë¸ í•™ìŠµÂ·ê²€ì¦ ìë™í™” | ì†ŒìŠ¤ ì½”ë“œ ë¹Œë“œÂ·í…ŒìŠ¤íŠ¸Â·ë°°í¬ ìë™í™”ê¹Œì§€ í¬í•¨ |
| **ì—°ì† í•™ìŠµ(CT)** | X | O (ìŠ¤ì¼€ì¤„/íŠ¸ë¦¬ê±° ê¸°ë°˜) | O (CT + ì½”ë“œ ë³€ê²½ ì‹œ ìë™ ë°˜ì˜) |
| **CI (ì§€ì†ì  í†µí•©)** | X | X | O (ì½”ë“œ ì»¤ë°‹ ì‹œ ë¹Œë“œÂ·í…ŒìŠ¤íŠ¸ ìë™ ì‹¤í–‰) |
| **CD (ì§€ì†ì  ë°°í¬)** | X | ëª¨ë¸ ë°°í¬ ìë™í™”ë§Œ | O (ëª¨ë¸Â·íŒŒì´í”„ë¼ì¸ ë°°í¬ ëª¨ë‘ ìë™í™”) |
| **ë°°í¬ ë‹¨ìœ„** | ëª¨ë¸ ì•„í‹°íŒ©íŠ¸ | ì „ì²´ í•™ìŠµ íŒŒì´í”„ë¼ì¸ | ì „ì²´ íŒŒì´í”„ë¼ì¸ + ëª¨ë¸ ì˜ˆì¸¡ ì„œë¹„ìŠ¤ |
| **ë°ì´í„°Â·ëª¨ë¸ ê²€ì¦** | ìˆ˜ë™ | ìë™ ë°ì´í„° ê²€ì¦Â·ëª¨ë¸ ì„±ëŠ¥ ê²€ì¦ | ìë™ ê²€ì¦ + ì„±ëŠ¥/í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸ |
| **íŠ¹ì§•ì  ì»´í¬ë„ŒíŠ¸** | ì—†ìŒ | íŒŒì´í”„ë¼ì¸ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°, ë°ì´í„°Â·ëª¨ë¸ ê²€ì¦, (ì˜µì…˜) í”¼ì²˜ ìŠ¤í† ì–´ | Level 1 êµ¬ì„± + ì†ŒìŠ¤ ì»¨íŠ¸ë¡¤, ë¹Œë“œ/í…ŒìŠ¤íŠ¸ ì„œë¹„ìŠ¤, ë°°í¬ ì„œë¹„ìŠ¤, ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ |
| **ë°°í¬ ì†ë„** | ë§¤ìš° ëŠë¦¼ (ì—° 1~2íšŒ) | ì¤‘ê°„ (ë°ì´í„° ë³€í™” ì‹œ ìë™ í•™ìŠµÂ·ë°°í¬) | ë§¤ìš° ë¹ ë¦„ (ë°ì´í„°Â·ì½”ë“œ ë³€í™” ëª¨ë‘ ìë™ ë°˜ì˜) |
| **ìš´ì˜ ëª¨ë‹ˆí„°ë§** | ì—†ìŒ | ëª¨ë¸ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ O | ëª¨ë¸ + íŒŒì´í”„ë¼ì¸ ëª¨ë‹ˆí„°ë§ O |
| **ì í•©í•œ ìƒí™©** | ëª¨ë¸ ë³€ê²½Â·ì¬í•™ìŠµ ê±°ì˜ ì—†ëŠ” ì´ˆê¸° ë‹¨ê³„ | ë°ì´í„° ë³€í™”ì— ìì£¼ ëŒ€ì‘í•´ì•¼ í•˜ëŠ” ê²½ìš° | ë°ì´í„°Â·ì½”ë“œ ëª¨ë‘ ìì£¼ ë³€ê²½ë˜ëŠ” ê³ ë„í™” í™˜ê²½ |