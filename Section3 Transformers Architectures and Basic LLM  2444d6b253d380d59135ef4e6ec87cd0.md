# Section3: Transformers Architectures and Basic LLM Concepts

### üîë Key Points: Introduction to Transformers via Sequence-to-Sequence Models

### 1. **Course Structure (High-Level)**:

- Start with **sequence-to-sequence (seq2seq)** models.
- Understand **why Transformers are needed** (limitations of RNNs/LSTMs).
- Learn about **attention mechanisms**.
- Finally, move into the **Transformer architecture**.

---

### 2. **What is Sequence-to-Sequence (Seq2Seq)?**

- A model architecture introduced by Google in **2014** for tasks like **machine translation**.
- Consists of:
    - **Encoder**: Takes input sequence (e.g., sentence in English).
    - **Decoder**: Produces output sequence (e.g., sentence in French).

---

### 3. **Key Features of Seq2Seq**:

- **Input and output can have different lengths**.
    - E.g., English sentence with 5 words ‚Üí French sentence with 4 words.
- Originally used **RNNs**, later replaced by **LSTMs/GRUs** to deal with:
    - **Vanishing/exploding gradient problems**.
- **Processes data sequentially**:
    - Word-by-word, which slows down training/inference.
- Has a **fixed-length context vector** (a bottleneck):
    - Entire input is compressed into a single hidden state before decoding.
    - This limits performance on **long sequences**.

---

### 4. **Why Seq2Seq Matters for Transformers**:

- Transformers improve on seq2seq by:
    - Allowing **parallel processing**.
    - Replacing the fixed context with **attention** (dynamic access to all input tokens).
- Understanding seq2seq helps you appreciate **how Transformers solve key problems**.

---

### üîë Key Points: Sequence-to-Sequence Encoder and Decoder (LSTM-based)

---

### üß© **1. Encoder (LSTM-based)**

- **Input**: A sequence of words ‚Üí tokenized ‚Üí passed through **embedding layers**.
- Each word embedding is processed **sequentially** through the **LSTM**.
- LSTM maintains and updates a **hidden state** `h‚ÇÅ ‚Üí h‚ÇÇ ‚Üí h‚ÇÉ ‚Üí h‚ÇÑ` (for 4 tokens, as example).
- **Final hidden state (`h‚ÇÑ`)** is used to summarize all input ‚Äî it's the **context vector** passed to the decoder.

‚ö†Ô∏è **Issue**:

- All the information from the input sequence is **compressed into one fixed-length hidden state (`h‚ÇÑ`)**.
- As input length increases, this **compression causes information loss**.
- Longer inputs = more loss ‚Üí **performance degradation**.

---

### üß© **2. Decoder (LSTM-based)**

- Starts with a **Start-of-Sequence (SOS)** token ‚Üí embedded and passed to LSTM.
- Uses the **final encoder hidden state (`h‚ÇÑ`)** as input for the initial state.
- At each step:
    - Takes the **previous output word** and **hidden state** to predict the **next word**.
- Stops once **End-of-Sequence (EOS)** token is generated.

‚ö†Ô∏è **Issues**:

- Like the encoder, decoder works **sequentially**, which:
    - Prevents **parallelization**.
    - Slows down training and inference.
- Still relies on a **single hidden state** as context ‚Üí same **bottleneck problem**.

---

### üö´ Limitations of LSTM-based Seq2Seq:

| Limitation | Explanation |
| --- | --- |
| **Compression Bottleneck** | Entire input is encoded into a single vector (`h‚ÇÑ`). |
| **Information Loss** | Long sequences = more loss during compression. |
| **No Parallelism** | Processing is strictly sequential (both in encoder and decoder). |
| **Fixed Memory Size** | Context vector is fixed in size regardless of input length. |

---

### ‚úÖ Motivation for Attention / Transformers:

- Instead of using **only the last hidden state**, **use all encoder hidden states** (`h‚ÇÅ, h‚ÇÇ, h‚ÇÉ, h‚ÇÑ`).
- Decoder can **dynamically "attend" to different parts** of the input as needed.
- This reduces:
    - **Compression loss**
    - **Sequential bottlenecks**
    - **Training time**

This sets up the motivation for **attention mechanisms**, which we‚Äôll cover next.

### üîë Key Concepts: Evolving Seq2Seq with Attention

---

### üí° Problem in Vanilla Seq2Seq (LSTM):

- Previously, only the **final hidden state** (e.g., `h‚ÇÑ`) from the encoder was passed to the decoder.
- This caused:
    - **Information bottleneck**
    - **Loss of detail** in longer sequences
    - Limited decoder performance

---

### ‚úÖ Solution ‚Äì Use **All Encoder Hidden States**:

- Instead of passing only `h‚ÇÑ`, pass **all hidden states**: `h‚ÇÅ, h‚ÇÇ, h‚ÇÉ, h‚ÇÑ` to the decoder.
- This allows the decoder to **selectively focus** on the most relevant parts of the input sequence at each output step.

> Think of it as giving the decoder a full map instead of a single summary.
> 

---

### üß† How Does the Decoder Use These States?

1. **Each decoder step** (e.g., generating one word) can **attend** to different encoder hidden states.
    - For example, when generating the second output word, it may focus more on `h‚ÇÇ`.
2. These encoder hidden states can be combined in various ways:
    - **Concatenation**
    - **Summation**
    - **Averaging**
    - Or via a learned **attention mechanism** (preferred)

---

### üß≠ Two Types of Attention to Understand:

| Type | What it Does |
| --- | --- |
| **Cross-Attention** | Decoder attends to the **encoder hidden states** |
| **Self-Attention** | A module (encoder or decoder) attends to **its own input tokens** |

---

### üöÄ Why This Matters:

- Enables the model to **dynamically use the full context**.
- **Reduces compression loss**.
- Supports **parallel computation** when used in Transformer architecture.
- Leads to **much better performance**, especially on longer sequences and complex tasks.

### üîë Key Concepts: Attention Mechanism in Sequence-to-Sequence Models

---

### üß© 1. **Problem Recap ‚Äì Traditional Seq2Seq with LSTM**

- Encoder produces **final hidden state (`h‚ÇÑ`)** as a compressed representation of all inputs.
- Issues:
    - **Information loss** for long sequences (compression bottleneck)
    - **Vanishing gradients** in long input chains
    - Cannot dynamically focus on specific parts of input
    - Decoding depends on a **fixed-length context vector**

---

### ‚úÖ 2. **Solution ‚Äì Use Attention Mechanism**

### ‚û§ Instead of using just `h‚ÇÑ`, pass **all encoder hidden states** (`h‚ÇÅ, h‚ÇÇ, h‚ÇÉ, h‚ÇÑ`) to the decoder.

- Decoder **decides which part to focus on** dynamically, per output token.

---

### üéØ 3. **Types of Attention**

| Type | What it Focuses On |
| --- | --- |
| **Self-Attention** | Within the same sequence (e.g., decoder ‚Üí decoder) |
| **Cross-Attention** | Decoder focuses on encoder hidden states |

---

### üìä 4. **Performance Gains with Attention**

- Traditional Seq2Seq performance **drops sharply** as sentence length increases.
- Attention-based models **maintain performance** even with longer sequences.
- Shown in the **original attention paper (2014/2015)**, before Transformers (2017).

---

### üìè 5. **Evaluation Metrics for Text Generation**

| Metric | Focus | Use Case |
| --- | --- | --- |
| **BLEU** | Precision | Translation, summarization |
| **ROUGE** | Recall | Summarization, generation |
| **F1 Score** | Balance of precision and recall | Combined metric |
- `F1 = 2 √ó (BLEU √ó ROUGE) / (BLEU + ROUGE)`

---

### üß† 6. **How Attention Weighs Inputs**

### ‚ùå *Naive Approach*:

- Just **add or average** all encoder hidden states:
    
    `C = h‚ÇÅ + h‚ÇÇ + h‚ÇÉ + h‚ÇÑ`
    
- Problem: No focus control ‚Äî all hidden states are equally weighted.

### ‚úÖ *Weighted Attention*:

- Use **weights (`w‚ÇÅ, w‚ÇÇ, w‚ÇÉ, w‚ÇÑ`)** to assign **importance** to each hidden state:
    
    ```
    ini
    Î≥µÏÇ¨Ìé∏Ïßë
    C = w‚ÇÅ¬∑h‚ÇÅ + w‚ÇÇ¬∑h‚ÇÇ + w‚ÇÉ¬∑h‚ÇÉ + w‚ÇÑ¬∑h‚ÇÑ
    
    ```
    
- Weights (`w`) are calculated using the **decoder‚Äôs previous hidden state** (so attention is dynamic).
- This lets the decoder **focus more on relevant tokens** in the input (e.g., `h‚ÇÉ`) during generation.

---

### üöÄ Next Step: Transformers

- Transformers **replace RNNs** entirely.
- Use **only attention mechanisms** (no recurrence).
- Introduce concepts like:
    - **Multi-head attention**
    - **Positional encoding**
    - **Layer normalization**

### üîë Key Concepts: Queries, Keys, Values, and Attention

---

### üìò Background

- **‚ÄúAttention is All You Need‚Äù** (2017): Introduced the **Transformer** architecture.
- Builds on earlier **attention mechanisms** from 2014‚Äì2015 that enhanced Seq2Seq models.

---

### üß© Core Components: Query, Key, Value (Q, K, V)

- Each token in a sentence is **mapped to three vectors**:
    - **Query (Q)**: What are we looking for?
    - **Key (K)**: What do we have?
    - **Value (V)**: What information do we retrieve?

> üîç Analogy:
> 
> - **Query** = your search input
> - **Keys** = titles & metadata of documents
> - **Values** = the actual content returned

---

### ‚öôÔ∏è How Attention Works (Scaled Dot-Product Attention)

Given a query vector **Q**, attention is computed over all **K-V pairs**:

1. **Score**: Compute similarity between the query and each key:score=Q‚ãÖKT
    
    score=Q‚ãÖKT\text{score} = Q \cdot K^T
    
    (This is a dot product.)
    
2. **Scale**: To prevent large dot product values from destabilizing softmax:scaled¬†score=dkQ‚ãÖKT
    
    scaled¬†score=Q‚ãÖKTdk\text{scaled score} = \frac{Q \cdot K^T}{\sqrt{d_k}}
    
3. **Softmax**: Convert the scores into **attention weights**:Œ±=softmax(dkQ‚ãÖKT)
    
    Œ±=softmax(Q‚ãÖKTdk)\alpha = \text{softmax}\left(\frac{Q \cdot K^T}{\sqrt{d_k}}\right)
    
4. **Weighted Sum**: Multiply each value **V** by its corresponding weight and sum:output=‚àëŒ±i‚ãÖVi
    
    output=‚àëŒ±i‚ãÖVi\text{output} = \sum \alpha_i \cdot V_i
    

---

### üîÅ Self-Attention vs Cross-Attention

| Type | Meaning |
| --- | --- |
| **Self-Attention** | Q, K, V all come from the **same sequence** (e.g., encoder attends to itself) |
| **Cross-Attention** | Q comes from decoder; K and V come from encoder (e.g., decoder attends to encoder output) |

---

### üß† Why Attention Helps

- **No bottleneck** like in LSTM-based Seq2Seq (no single context vector).
- **Long sequences** handled better.
- Enables **parallelization** of computation.
- Learns **which parts of input are relevant** for generating each output token.

---

### üìä Evaluation Metrics Mentioned

| Metric | Focus | Use Case |
| --- | --- | --- |
| **BLEU** | Precision | Common in translation tasks |
| **ROUGE** | Recall | Common in summarization |
| **F1** | Combined | 2‚ãÖBLEU‚ãÖROUGEBLEU+ROUGE2 \cdot \frac{\text{BLEU} \cdot \text{ROUGE}}{\text{BLEU} + \text{ROUGE}}2‚ãÖBLEU+ROUGEBLEU‚ãÖROUGE |

### üîë Key Concept: **Scaled Dot-Product Attention** (from *Attention Is All You Need*)

---

### ‚öôÔ∏è **1. The Core Attention Formula**

Attention(Q,K,V)=softmax(QKTdk)V\text{Attention}(Q, K, V) = \text{softmax}\left( \frac{QK^T}{\sqrt{d_k}} \right) V

Attention(Q,K,V)=softmax(dk

QKT)V

- **Q** = Queries
- **K** = Keys
- **V** = Values
- **dkd_kdk** = Dimension of key vectors (used to scale)

---

### üß† 2. **Why Scaling?**

- Dot products of large vectors can produce **very large values**.
- Applying softmax on large values can lead to **very small gradients** ‚Üí training becomes unstable.
- So we **scale** by dk\sqrt{d_k}dk to keep the softmax values in a stable range.

---

### üß© 3. **Where Do Q, K, V Come From?**

- All are derived from **input embeddings** using **learned linear layers**:Q=XWQ,K=XWK,V=XWV
    
    Q=XWQ,K=XWK,V=XWVQ = XW^Q,\quad K = XW^K,\quad V = XW^V
    
- WQW^QWQ, WKW^KWK, WVW^VWV are **learned weight matrices** (i.e., neural layers).
- Multiple such transformations allow the model to capture **different linguistic aspects** (gender, syntax, POS, etc.)

---

### üß† 4. **Masked Attention (Optional)**

- Used in **decoder during training** to prevent attention on **future tokens**.
- Ensures autoregressive generation.
- Implemented by setting future scores to **‚àû\infty‚àû** before applying softmax.

---

### üéØ 5. **Flexible Focus ‚Äì Not Just Diagonal!**

- Attention is **not restricted to word-aligned positions**.
- A word in position 4 can attend strongly to position 0, 6, etc.
- This is powerful for:
    - **Long-range dependencies**
    - **Reordering in translation**
    - **Coreference resolution**

---

### üß™ 6. **Visualization**

- Attention is often visualized as a heatmap:
    - Rows: output tokens (queries)
    - Columns: input tokens (keys)
    - Brightness: attention weight (how much that input is focused on)

Example:

```
vbnet
Î≥µÏÇ¨Ìé∏Ïßë
Query: "t"
Focuses on Input: "tea"
‚Üí High attention weight

```

In **practice**, attention maps **do not follow diagonal lines**, proving that the model learns **flexible, non-local relationships**.

---

### üß† 7. **Multi-Head Attention (Coming Next)**

- Multiple Q-K-V projections allow the model to attend to different features **in parallel**.
- Enables capturing **multiple types of dependencies** (e.g., grammar, position, meaning).

---

### ‚úÖ Summary

| Component | Role |
| --- | --- |
| **Q (Query)** | What am I looking for? |
| **K (Key)** | What do I have? |
| **V (Value)** | What info do I return if there's a match? |
| **Softmax(Q¬∑K·µÄ / ‚àöd‚Çñ)** | How similar is the query to each key (attention weights)? |
| **Multiply by V** | Get the final representation for the query's context |

## üîç Overview: What is a Transformer?

- Introduced in the paper **‚ÄúAttention is All You Need‚Äù** (2017).
- Replaces recurrence (RNN/LSTM) with **self-attention** and **parallel processing**.
- Major parts:
    - **Encoder stack** (understands input)
    - **Decoder stack** (generates output)

---

## üß© 1. Input Processing (Encoder Side)

### Example sentence: `"I love coffee"`

### Step-by-step:

1. **Tokenization**
    
    ‚Üí Breaks into tokens: `["I", "love", "coffee"]`
    
2. **Token Encoding**
    
    ‚Üí Assigns IDs: `[101, 2071, 4513]` (example IDs)
    
3. **Embedding**
    
    ‚Üí Maps token IDs to vectors (using learned embeddings)
    
4. **Positional Encoding**
    
    ‚Üí Adds position info (since Transformers are not sequential by default)
    
5. **Input to Encoder Stack**
    
    ‚Üí Final input = word embedding + positional encoding
    

---

- E.g., index 0 gets a unique vector, index 1 gets a different one, etc.

## üß† 2. Encoder Stack

Each encoder layer contains:

1. **Multi-head self-attention**
    
    ‚Üí Each word can focus on other words (even itself)
    
2. **Feed-forward neural network**
    
    ‚Üí Applied to each token vector independently
    
3. **Add & Normalize**
    
    ‚Üí Stabilizes training
    

### Result:

- Produces a **contextualized vector** for each input token.
    - E.g., ‚Äúcoffee‚Äù now knows it‚Äôs in the context of "I love"

---

## üì§ 3. Output Processing (Decoder Side)

Let‚Äôs say you‚Äôre doing **machine translation**:

Input: `"I love coffee"`

Expected Output: `"J'aime le caf√©"`

### Decoder Stack Steps:

1. **Start with [START] token**
    
    ‚Üí Decoder begins with `[START]` and empty context
    
2. **Embedding + Positional Encoding**
    
    ‚Üí Same as in encoder
    
3. **Masked Multi-head self-attention**
    
    ‚Üí Prevents the model from seeing ‚Äúfuture‚Äù words during training (e.g., no peeking at ‚Äúcaf√©‚Äù while generating ‚ÄúJ'aime‚Äù)
    
4. **Cross-attention**
    
    ‚Üí Decoder looks at encoder output (context of input sentence)
    
5. **Final feed-forward + softmax**
    
    ‚Üí Outputs probabilities over the vocabulary for the next word
    
6. **Prediction**
    
    ‚Üí Choose the word with highest probability (or use sampling strategies)
    

---

## üì¶ 4. How Attention Connects Encoder and Decoder

### Key Idea:

- Decoder needs to ‚Äúpay attention‚Äù to relevant words in the input sentence.

So:

- **Encoder output** = keys and values
- **Decoder hidden state** = query

This allows the decoder to **dynamically attend to the right parts of the input** for each output token.

---

## üí° Why This Is Powerful

| Benefit | Explanation |
| --- | --- |
| **Parallelization** | All tokens can be processed at once (great for GPUs) |
| **No recurrence** | No more slow, step-by-step sequence modeling |
| **Flexible attention** | Any word can attend to any other word (not just nearby) |
| **Better long-sequence modeling** | Handles long context far better than LSTM or GRU |

---

## üîÅ Summary Flow

**Encoder side**:

```
Input ‚Üí Tokenize ‚Üí Embed ‚Üí Add Position ‚Üí Encoder stack ‚Üí Contextual vectors
```

**Decoder side**:

```
Start token ‚Üí Embed ‚Üí Add Position
 ‚Üí Masked self-attention
 ‚Üí Cross-attention (with encoder output)
 ‚Üí FFN ‚Üí Softmax ‚Üí Predict next token
```

## üîë **What Is the Encoder Doing?**

**Goal**: Take a sentence (like "I am not going") and turn it into **contextualized vectors** ‚Äî where each word is represented not just by itself, but by **what it means in context**.

---

## üß© **Step-by-Step Encoder Process**

Let‚Äôs use this example sentence:

> "I am not going"
> 

---

### üîπ Step 1: Input Tokenization

- Split into tokens: `["I", "am", "not", "going"]`

---

### üîπ Step 2: Embedding

- Each token gets converted to a vector (via learned embeddings):
    
    ```
    css
    Î≥µÏÇ¨Ìé∏Ïßë
    I      ‚Üí [0.1, 0.3, ...]
    am     ‚Üí [0.5, 0.2, ...]
    not    ‚Üí [0.9, 0.6, ...]
    going  ‚Üí [0.3, 0.8, ...]
    
    ```
    
- These vectors go into the encoder.

---

### üîπ Step 3: Add Positional Encoding

- Since Transformers are not sequential by default, we **add position info** to each token vector.
    - E.g., token at position 0 is treated differently from token at position 3.

---

### üîπ Step 4: Calculate Q, K, V for Each Token

- From each word vector (embedding), we compute:
    - **Q** = query vector
    - **K** = key vector
    - **V** = value vector
- These are calculated using **learned linear layers**:
    
    ```
    ini
    Î≥µÏÇ¨Ìé∏Ïßë
    Q = X √ó W_Q
    K = X √ó W_K
    V = X √ó W_V
    
    ```
    
- Example for ‚Äúgoing‚Äù:
    
    ```
    ini
    Î≥µÏÇ¨Ìé∏Ïßë
    Q_going = embedding("going") √ó W_Q
    
    ```
    

---

### üîπ Step 5: **Self-Attention** in Encoder

Each word **attends** to all other words ‚Äî including itself.

Let‚Äôs say we want to re-represent the word **‚Äúgoing‚Äù**:

> How important are ‚ÄúI‚Äù, ‚Äúam‚Äù, and ‚Äúnot‚Äù when processing ‚Äúgoing‚Äù?
> 
1. Compute similarity between **Q_going** and each **K_i** (from all tokens).
2. Apply **softmax** to get attention weights.
3. Multiply each **V_i** (value vector) by the corresponding weight.
4. Sum them up to get the **new vector for ‚Äúgoing‚Äù**, now aware of its context.

---

### üß† Example Output:

> ‚Äúgoing‚Äù becomes:
> 

```
ini
Î≥µÏÇ¨Ìé∏Ïßë
new_vector = 0.1√óV_I + 0.3√óV_am + 0.6√óV_not + 0.0√óV_going

```

This tells the model:

- "not" is **highly relevant** to "going" (e.g., "not going").
- So the model builds a representation of "going" that reflects **negation**.

---

### üî∏ Why Multi-Head Attention?

Instead of doing this attention just **once**, we do it **multiple times in parallel**, each with its own Q/K/V projections.

Each head can focus on:

- Head 1: Syntax
- Head 2: Subject-object relation
- Head 3: Negation
- Head 4: Time indicators

At the end, the outputs of all heads are **concatenated** and passed through a **feed-forward neural network**.

---

## ‚úÖ Final Output of Encoder

- For each token, you get a **contextualized vector**.
- Now "going" doesn‚Äôt just mean ‚Äúto move‚Äù ‚Äî it reflects its **context**, like ‚Äúnot going‚Äù.

These **context-rich vectors** go to the **decoder**, or are used for classification (in BERT-like models).

---

## üìå Summary of Encoder Flow:

| Step | Description |
| --- | --- |
| 1. Tokenization | Break text into tokens |
| 2. Embedding + Positional Encoding | Convert tokens into vectors and add position info |
| 3. Q/K/V Calculation | Linear projections from input vectors |
| 4. Multi-head Self-Attention | Each token attends to all others |
| 5. Feed-Forward Network | Applies non-linearity to each token vector |
| 6. Output | Contextual vectors for each token |

## üß† Why Do We Need Positional Encoding?

Transformers **do not have recurrence** (like RNNs) or convolution (like CNNs).

That means:

‚û°Ô∏è They treat each word **independently** unless we explicitly give them **position information**.

Without position info:

> "I love you" and "you love I" would look the same to the model!
> 

---

## üìò The Idea of Positional Encoding

Instead of learning positions (which adds more parameters), the authors used a **static mathematical function**:

- Uses **sine and cosine** functions
- Gives a **unique vector** for each position in the sequence
- Adds it to the word embedding so the model knows **where the word is**

---

## üßÆ Formula (From Paper)

For a position `pos` and dimension index `i`, the positional encoding vector is:

PE(pos,2i)=sin‚Å°(pos100002i/dmodel)\text{PE}_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i / d_{\text{model}}}}\right)

PE(pos,2i)=sin(100002i/dmodelpos)

PE(pos,2i+1)=cos‚Å°(pos100002i/dmodel)\text{PE}_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i / d_{\text{model}}}}\right)

PE(pos,2i+1)=cos(100002i/dmodelpos)

- `d_model` = the total dimension of the embedding (e.g., 512)
- Even indices ‚Üí use **sine**
- Odd indices ‚Üí use **cosine**

---

## üéØ What This Does

- **Low-frequency waves** capture **coarse** position (early layers)
- **High-frequency waves** capture **fine-grained** position (later layers)
- Together, they provide a **rich, unique pattern** per position that helps attention know ‚Äúwhere‚Äù tokens are.

---

## üìä Example: Simple 4-Dimensional Positional Encoding

Let‚Äôs say:

- `d_model = 4` (small for simplicity)
- Sentence: `"I am not going"`
- Positions = 0, 1, 2, 3

### üîπ For position = 0:

PE(0)=[sin‚Å°(0),cos‚Å°(0),sin‚Å°(0),cos‚Å°(0)]=[0,1,0,1]\text{PE}(0) = [\sin(0), \cos(0), \sin(0), \cos(0)] = [0, 1, 0, 1]

PE(0)=[sin(0),cos(0),sin(0),cos(0)]=[0,1,0,1]

### üîπ For position = 1:

You apply the formula with different divisors (based on `10000^(2i/d_model)`)

Let‚Äôs compute approximate values (rounded):

PE(1)=[sin‚Å°(1/1),cos‚Å°(1/1),sin‚Å°(1/100),cos‚Å°(1/100)]‚âà[sin‚Å°(1),cos‚Å°(1),sin‚Å°(0.01),cos‚Å°(0.01)]‚âà[0.84,0.54,0.01,0.9999]\text{PE}(1) = [\sin(1/1), \cos(1/1), \sin(1/100), \cos(1/100)]
\approx [\sin(1), \cos(1), \sin(0.01), \cos(0.01)]
\approx [0.84, 0.54, 0.01, 0.9999]

PE(1)=[sin(1/1),cos(1/1),sin(1/100),cos(1/100)]‚âà[sin(1),cos(1),sin(0.01),cos(0.01)]‚âà[0.84,0.54,0.01,0.9999]

You‚Äôll notice that:

- First two dimensions change more (low-frequency)
- Later ones change less (high-frequency)

---

## üìå Key Properties

| Feature | Explanation |
| --- | --- |
| **No training** | It's fixed (no learnable parameters) |
| **Fast** | No extra computation during training |
| **Unique** | Each position has a distinct pattern |
| **Similar positions are similar** | Helps capture distance/direction |
| **Works well in practice** | Learned embeddings didn't show significant improvement |

---

## üß† Analogy

Imagine you're writing music:

- Each **note** (word) has its **pitch** (embedding)
- But to make a melody, you also need to know **where it lands on the beat** (position)

Positional encoding adds **rhythm** to the **sound** ‚Äî without it, words are just floating in space.

---

## ‚úÖ Summary

| Without Positional Encoding | With Positional Encoding |
| --- | --- |
| Words have no sense of order | Position info is encoded into the vector |
| All word embeddings are same regardless of position | Same word gets slightly different vector at each position |
| Can‚Äôt handle sequence tasks | Can model syntax and structure well |

## üîç Three Types of Attention in the Transformer

| Type | Where it occurs | What it does |
| --- | --- | --- |
| **Self-Attention** | Encoder & Decoder | Helps a word focus on other words in the same sequence |
| **Masked Self-Attention** | Decoder only | Prevents peeking into future words while generating output |
| **Cross-Attention (Encoder‚ÄìDecoder Attention)** | Decoder | Helps output tokens attend to the input tokens (from encoder) |

---

## üß† 1. **Self-Attention** (Used in **Encoder** and **Decoder**)

### üí¨ Example: `"It's time for tea"`

Each word builds its **contextualized representation** by attending to all words **in the same sentence**:

| Predicting "tea" ‚Üí attention weights might be: |

|--------------|---------------------|

| "It's"       | 0.2                 |

| "time"       | 0.1                 |

| "for"        | 0.1                 |

| **"tea"**    | **0.6**             |

So the model learns that "tea" heavily relates to itself but might also consider "for".

### üîÅ Happens in **parallel**:

Every word computes self-attention **simultaneously**, not sequentially.

---

## üîê 2. **Masked Self-Attention** (Used **only in Decoder** during training)

### ü§î Why "Masked"?

When generating output like:

> "Je t'aime beaucoup"
> 

At each step, the model should **not look ahead**.

So, when predicting `"t'aime"` (step 2), the model **can only use previous tokens**:

```
cpp
Î≥µÏÇ¨Ìé∏Ïßë
Input so far: ["Je", "t'aime"]
Mask: ‚úîÔ∏è "Je", ‚úîÔ∏è "t'aime", ‚ùå future words

```

### üìâ How masking works:

In attention score matrix (Q¬∑K·µÄ), we **fill upper triangle** with **‚àí‚àû**, so that softmax = 0 (i.e., no attention to future tokens).

|  | Je | t'aime | beaucoup |
| --- | --- | --- | --- |
| Je | ‚úîÔ∏è | ‚ùå | ‚ùå |
| t'aime | ‚úîÔ∏è | ‚úîÔ∏è | ‚ùå |
| beaucoup | ‚úîÔ∏è | ‚úîÔ∏è | ‚úîÔ∏è |

---

## üîÑ 3. **Cross-Attention** = Encoder‚ÄìDecoder Attention

### üéØ When?

- After masked self-attention in the decoder.
- Decoder uses encoder outputs (the **contextual embeddings**) as its **Key (K)** and **Value (V)**.
- Decoder‚Äôs own hidden state becomes the **Query (Q)**.

### üí¨ Example:

**Input (Encoder)**: `"I love coffee"`

**Output (Decoder)**: `"J'aime le caf√©"`

While generating `"caf√©"`, the decoder attends to `"coffee"` from the encoder.

| Decoder token ‚Üí | Attends to Encoder tokens |
| --- | --- |
| `"caf√©"` | High weight on `"coffee"` |
| `"le"` | Medium weight on `"the"` |
| `"J'aime"` | Mix of `"I"` and `"love"` |

---

## ‚öôÔ∏è How It All Connects

In the **Transformer decoder**, for each output token:

1. **Masked Self-Attention**: Look at **past output tokens** only
2. **Cross-Attention**: Attend to **input sentence** (via encoder outputs)
3. **Feedforward & Softmax**: Generate next token probabilities

---

## üß† Key Takeaways

| Type | Used In | Attends To | Purpose |
| --- | --- | --- | --- |
| Self-Attention | Encoder & Decoder | Same sequence | Learn relationships within a sequence |
| Masked Self-Attention | Decoder | Past decoder tokens only | Prevent peeking at future tokens |
| Cross-Attention | Decoder | Encoder outputs | Link output words to input context |

---

## üí° Final Analogy

Imagine translating a sentence:

1. First, you understand the whole input sentence ‚Üí **Encoder with Self-Attention**
2. Then you start writing your translation one word at a time ‚Üí **Decoder**
    - You remember what you've already written ‚Üí **Masked Self-Attention**
    - You keep referring back to the original sentence ‚Üí **Cross-Attention**

## üéØ Why Multi-Head Attention (MHA)?

> Instead of attending to information from just one perspective, MHA allows the model to attend to multiple perspectives in parallel.
> 

### üß† Analogy:

Think of **reading a sentence with 8 different highlighters** ‚Äî each highlighter focuses on a different aspect:

- Highlighter 1: subject‚Äìverb relationships
- Highlighter 2: noun modifiers
- Highlighter 3: pronouns and antecedents
- ‚Ä¶
- Highlighter 8: word tense

---

## üîß What Is It?

Multi-head attention = **several attention heads in parallel**, each computing scaled dot-product attention with different learnable weights.

Each head learns to attend to different parts or aspects of the input.

---

## üîç Let‚Äôs Break It Down with an Example

### Input Sentence:

```
arduino
Î≥µÏÇ¨Ìé∏Ïßë
"I am not going"

```

Assume:

- Model dimension `d_model = 512`
- Number of heads `h = 8`
- So each head gets a slice of size `d_k = d_v = 64` (since 512 / 8 = 64)

### Step-by-Step:

### Step 1: Linear Projection to Q, K, V (per head)

For each head, we:

- Linearly project input embeddings to get:
    
    ```
    bash
    Î≥µÏÇ¨Ìé∏Ïßë
    Q‚ÇÅ, K‚ÇÅ, V‚ÇÅ (for head 1)
    Q‚ÇÇ, K‚ÇÇ, V‚ÇÇ (for head 2)
    ...
    Q‚Çà, K‚Çà, V‚Çà (for head 8)
    
    ```
    

This is done using learned weight matrices:

```python
python
Î≥µÏÇ¨Ìé∏Ïßë
Q‚ÇÅ = X @ Wq‚ÇÅ      # shape: (seq_len, 64)
K‚ÇÅ = X @ Wk‚ÇÅ
V‚ÇÅ = X @ Wv‚ÇÅ

```

These weights (`Wq`, `Wk`, `Wv`) are different for **each head**, and they are **learned during training**.

### Step 2: Apply Scaled Dot-Product Attention (per head)

Each head computes:

```python
python
Î≥µÏÇ¨Ìé∏Ïßë
Attention‚ÇÅ = softmax(Q‚ÇÅ @ K‚ÇÅ.T / sqrt(64)) @ V‚ÇÅ

```

This gives a contextualized representation per head.

### Step 3: Concatenate the Outputs

You get 8 output matrices of shape `(seq_len, 64)`. Concatenate them:

```python
python
Î≥µÏÇ¨Ìé∏Ïßë
Concat = [head‚ÇÅ, head‚ÇÇ, ..., head‚Çà]    # shape: (seq_len, 512)

```

### Step 4: Final Linear Projection

The concatenated output is passed through another learned linear transformation:

```python
python
Î≥µÏÇ¨Ìé∏Ïßë
Final = Concat @ W_o    # shape: (seq_len, 512)

```

Where `W_o` is also learned during training.

---

## üí° Why Is This Powerful?

- Each head learns **different relationships** and **different linguistic structures**.
- Because dimensionality is divided across heads (e.g., 512 ‚Üí 8 heads √ó 64), **parameter count remains fixed**.
- Allows the model to **parallelize attention computation** and improves efficiency.
- Boosts **model expressiveness** without increasing cost.

---

## üìä Visualization

Here‚Äôs how attention might look across different heads:

| Head | Focus Pattern Example |
| --- | --- |
| 1 | Subject ‚Üí Verb |
| 2 | Adjective ‚Üí Noun |
| 3 | Pronoun ‚Üí Antecedent |
| 4 | Past vs. Present Tense |
| 5‚Äì8 | Other syntactic/semantic relations |

In each case, attention weights differ across heads ‚Äî and those weights are learned based on **what helps improve the prediction**.

---

## üìå Summary Diagram (Described Visually)

```
bash
Î≥µÏÇ¨Ìé∏Ïßë
Input X  ‚Üí  Linear Layers for Q, K, V (one set per head)
        ‚Üí  8 heads: [head‚ÇÅ, head‚ÇÇ, ..., head‚Çà]  (each = dot-product attention)
        ‚Üí  Concatenate all head outputs
        ‚Üí  Final linear layer (W_o)
        ‚Üí  Output: Contextual representation (same shape as input)

```

---

## ‚úÖ Final Thoughts

- Multi-head attention is **not just repeating attention** multiple times ‚Äî it's **diversifying attention** in **parallel projections**.
- By learning **multiple attention spaces**, MHA gives the Transformer model **depth and nuance** in how it understands language.

### üîë **Key Concepts from the Decoder in Transformers**

1. **Decoder Overview**
    
    The Transformer decoder is used for generating output sequences, typically in tasks like text generation or translation. It uses a stack of decoder layers and is key to autoregressive language models (e.g., GPT series).
    
2. **Decoder Components**
    
    Each decoder layer includes:
    
    - **Masked Multi-Head Self-Attention**:
        
        Prevents the model from attending to future tokens (ensures causal generation). Implemented using a mask that sets future token logits to `-‚àû` before softmax.
        
    - **Encoder-Decoder (Cross) Attention**:
        
        Allows the decoder to attend to encoder outputs. This is used only when both encoder and decoder are present (e.g., translation).
        
    - **Feed Forward Neural Network**:
        
        A fully connected layer for further transformation.
        
3. **Input Processing**
    - The decoder takes **tokenized** previous outputs.
    - Converts them into **embeddings**, adds **positional encodings** (using sinusoidal functions), and passes them through the decoder stack.
4. **Multi-Head Attention**
    - Multiple attention heads let the model capture different types of relationships and linguistic features in parallel.
    - The total dimensionality is split across heads to keep parameter count constant.
5. **Autoregressive Generation**
    - The decoder outputs a **probability distribution over all tokens** (typically ~50,000 vocabulary size).
    - **Next token prediction** is performed using decoding strategies like greedy sampling, top-k, top-p (nucleus), etc.
    - This process repeats by appending the predicted token to the input sequence for the next step.
6. **Decoder-Only Models**
    - GPT-2, GPT-3, GPT-4, Gemini, etc., are **decoder-only** Transformer models.
    - These are trained in an autoregressive manner to predict the next token, using **masked self-attention** only (no encoder involved).